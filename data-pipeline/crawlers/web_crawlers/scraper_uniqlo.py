import os
import asyncio
import re
import json
import glob
import pandas as pd
from datetime import datetime
from playwright.async_api import async_playwright
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

# --- ì„¤ì • ---
BRAND_NAME = "uniqlo"
LOCAL_OUTPUT_PATH = f"crawlers/data/{BRAND_NAME}_json_files"

TARGET_MAP = {
    "Men": {
        "Outer": [
            "https://www.uniqlo.com/kr/ko/men/outerwear/coats"
        ]
    }
}

visited_products = set()
sem = asyncio.Semaphore(3)

# Spark Schema
schema = StructType([
    StructField("gender", StringType(), True),
    StructField("category", StringType(), True),
    StructField("product_id", StringType(), True),
    StructField("raw_json", StringType(), True)
])

async def extract_product_data_from_dom(page, product_id):
    """ê°¤ëŸ¬ë¦¬ ê·¸ë¦¬ë“œ íƒ€ê²ŸíŒ…ìœ¼ë¡œ ì´ë¯¸ì§€ ì „ìˆ˜ ì¡°ì‚¬"""
    try:
        # [1] ë¡œë”© ìœ ë„ & ì•„ì½”ë””ì–¸ ê°•ì œ ê°œë°©
        await page.mouse.wheel(0, 2000)
        await asyncio.sleep(1.5)
        
        # CSS ê°•ì œ ì¡°ì‘ (ìˆ¨ê²¨ì§„ ë‚´ìš© í¼ì¹˜ê¸°)
        await page.evaluate("""() => {
            document.querySelectorAll('.rah-static, [aria-hidden="true"]').forEach(el => {
                el.style.display = 'block';
                el.style.height = 'auto';
                el.style.visibility = 'visible';
            });
            document.querySelectorAll('button').forEach(btn => {
                if(btn.innerText.includes('ìƒì„¸ ì„¤ëª…') || btn.innerText.includes('ì†Œì¬ ì •ë³´')) {
                    btn.click();
                }
            });
        }""")
        await asyncio.sleep(1.0)

        data = await page.evaluate("""() => {
            const result = {};

            // 1. ê¸°ë³¸ ì •ë³´
            const urlMatch = location.href.match(/\/products\/([A-Z0-9-]+)/);
            result.goodsNo = urlMatch ? urlMatch[1] : ""; 
            
            const bodyText = document.body.innerText;
            const productNoMatch = bodyText.match(/ì œí’ˆ ë²ˆí˜¸[:\\s]*([0-9]+)/);
            result.display_goods_no = productNoMatch ? productNoMatch[1] : "";

            const metaTitle = document.querySelector('meta[property="og:title"]')?.content;
            result.goodsNm = metaTitle ? metaTitle.split('|')[0].trim() : document.title;
            result.brandName = "UNIQLO";
            result.thumbnailImageUrl = document.querySelector('meta[property="og:image"]')?.content || "";

            // 2. ê°€ê²©
            let price = 0;
            const priceEl = document.querySelector('.fr-ec-price-text') || document.querySelector('.price');
            if (priceEl) price = parseInt(priceEl.innerText.replace(/[^0-9]/g, '') || "0");
            if (price === 0) {
                const ariaPrice = document.querySelector('.fr-ec-price');
                if (ariaPrice) {
                    const match = ariaPrice.getAttribute('aria-label').match(/([0-9,]+)ì›/);
                    if (match) price = parseInt(match[1].replace(/,/g, ''));
                }
            }
            result.price = price;

            // 3. í’ˆì ˆ ì—¬ë¶€
            let isSoldOut = false;
            if (price === 0) isSoldOut = true;
            document.querySelectorAll('button').forEach(btn => {
                const txt = btn.innerText;
                if ((txt.includes('êµ¬ë§¤') || txt.includes('ì¥ë°”êµ¬ë‹ˆ')) && btn.disabled) isSoldOut = true;
                if (txt.includes('í’ˆì ˆ')) isSoldOut = true;
            });
            result.is_sold_out = isSoldOut;

            // 4. ì‚¬ì´ì¦ˆ
            const sizeStockInfo = [];
            document.querySelectorAll('button.chip, label.chip').forEach(chip => {
                const name = chip.innerText.trim();
                if (name && name.length < 6 && /^[0-9A-Z]+$/.test(name)) {
                    let isItemSoldOut = chip.classList.contains('disabled') || chip.disabled || chip.getAttribute('aria-disabled') === 'true';
                    if (!sizeStockInfo.some(s => s.size === name)) {
                        sizeStockInfo.push({ size: name, is_sold_out: isItemSoldOut, stock_qty: isItemSoldOut ? 0 : 999 });
                    }
                }
            });
            result.size_stock = sizeStockInfo;

            // 5. ìƒ‰ìƒ
            const colors = [];
            document.querySelectorAll('.collection-list-horizontal button.chip, .color-chip button').forEach(btn => {
                const code = btn.value || (btn.id.split('-').length > 1 ? btn.id.split('-')[1] : ""); 
                const img = btn.querySelector('img');
                const name = img ? img.alt : ""; 
                const iconUrl = img ? img.src : "";
                if (code && name) {
                    colors.push({ color_code: code, color_name: name, icon_url: iconUrl });
                }
            });
            result.colors = colors;

            // ---------------------------------------------------------
            // [6] ì´ë¯¸ì§€ (ì‚¬ìš©ì ì œë³´ êµ¬ì¡° ë°˜ì˜: .media-gallery--grid)
            // ---------------------------------------------------------
            const images = [];
            
            // 1) ê°¤ëŸ¬ë¦¬ ê·¸ë¦¬ë“œ ë‚´ë¶€ ì´ë¯¸ì§€ ìš°ì„  íƒìƒ‰ (ì—¬ê¸°ì— ë‹¤ ëª¨ì—¬ìˆìŒ)
            const galleryImgs = document.querySelectorAll('.media-gallery--grid img');
            
            galleryImgs.forEach(img => {
                // data-srcê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (Lazy Loading ëŒ€ì‘), ì—†ìœ¼ë©´ src ì‚¬ìš©
                let src = img.getAttribute('data-src') || img.src;
                
                if (src && src.includes('uniqlo.com')) {
                    // ?width=600 ë“± ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ ì œê±°í•˜ì—¬ ì›ë³¸ í™”ì§ˆ í™•ë³´
                    src = src.split('?')[0];
                    images.push(src);
                }
            });

            // 2) ë§Œì•½ ê°¤ëŸ¬ë¦¬ì—ì„œ ëª» ì°¾ì•˜ìœ¼ë©´(êµ¬ì¡° ë³€ê²½ ë“±), í˜ì´ì§€ ì „ì²´ ì´ë¯¸ì§€ íƒìƒ‰ (ë°±ì—…)
            if (images.length === 0) {
                document.querySelectorAll('img').forEach(img => {
                    let src = img.src;
                    if (src && (src.includes('/goods/') || src.includes('/item/') || src.includes('/sub/')) && !src.includes('/chip/')) {
                        src = src.split('?')[0];
                        images.push(src);
                    }
                });
            }

            result.goodsImages = [...new Set(images)]; // ì¤‘ë³µ ì œê±°

            // ---------------------------------------------------------
            // 7. ìƒì„¸ ì •ë³´
            // ---------------------------------------------------------
            const detailedInfo = { "description": "", "material_info": {} };
            
            // ì„¤ëª… (ID ìš°ì„ )
            const descEl = document.getElementById('productLongDescription-content');
            if (descEl) {
                detailedInfo['description'] = descEl.innerText.replace(/\\n+/g, ' ').trim();
            } else {
                // í…ìŠ¤íŠ¸ ë§ˆì´ë‹
                const pTags = document.querySelectorAll('p, div.typography');
                let capture = false;
                for (const p of pTags) {
                    if (p.innerText.includes('ì œí’ˆ ìƒì„¸ ì„¤ëª…')) capture = true;
                    else if (p.innerText.includes('ì†Œì¬ ì •ë³´')) capture = false;
                    else if (capture && p.innerText.length > 20) {
                        detailedInfo['description'] += p.innerText + " ";
                    }
                }
            }

            // ì†Œì¬ (ID ìš°ì„ )
            const matEl = document.getElementById('productMaterialDescription-content');
            let rawMatText = matEl ? matEl.innerText : "";
            
            if (!rawMatText) {
                const bodyTxt = document.body.innerText;
                const match = bodyTxt.match(/ì†Œì¬ ì •ë³´[\\s\\S]{0,800}í’ˆì§ˆë³´ì¦ê¸°ì¤€/);
                if (match) rawMatText = match[0];
            }

            if (rawMatText) {
                const lines = rawMatText.split('\\n');
                let currentKey = null;
                const keywords = ['ì†Œì¬', 'ì„¸íƒ ë°©ë²•', 'ì œì¡°êµ­', 'ì œì¡°ì—°ì›”', 'ì œì¡°ì‚¬/ìˆ˜ì…ì', 'í’ˆì§ˆë³´ì¦ê¸°ì¤€'];
                
                lines.forEach(line => {
                    const txt = line.trim();
                    if (!txt) return;
                    if (keywords.includes(txt)) {
                        currentKey = txt;
                    } else if (currentKey) {
                        if (!detailedInfo['material_info'][currentKey]) {
                            detailedInfo['material_info'][currentKey] = txt;
                        } else {
                            detailedInfo['material_info'][currentKey] += " " + txt;
                        }
                    }
                });
            }
            result.goodsMaterial = detailedInfo;

            return result;
        }""")
        
        if not data or not data.get('goodsNm') or "Notice" in data.get('goodsNm', ''): 
            return None
            
        data['url'] = page.url
        data['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return data

    except Exception:
        return None

async def process_product(product_id, gender, category, context, collected_data):
    if product_id in visited_products: return
    visited_products.add(product_id)

    async with sem:
        clean_id = product_id.split('?')[0]
        url = f"https://www.uniqlo.com/kr/ko/products/{clean_id}"
        
        p_page = await context.new_page()
        try:
            print(f"   ğŸ” {clean_id} ì ‘ì† ì¤‘...")
            await p_page.goto(url, timeout=60000, wait_until="domcontentloaded")
            
            product_dict = None
            for _ in range(2):
                product_dict = await extract_product_data_from_dom(p_page, clean_id)
                if product_dict: break
                await asyncio.sleep(1)

            if product_dict:
                colors = product_dict.get('colors', [])
                # ìƒ‰ìƒì´ ìˆìœ¼ë©´ ë¶„í•  ì €ì¥, ì—†ìœ¼ë©´ ë‹¨ì¼ ì €ì¥
                if colors:
                    print(f"   ğŸ¨ {clean_id} ìƒ‰ìƒ {len(colors)}ê°œ ë°œê²¬")
                    for color in colors:
                        final_data = product_dict.copy()
                        final_data['color_name'] = color['color_name'] 
                        final_data['goodsNo'] = f"{clean_id}_{color['color_code']}" 
                        final_data['url'] = f"{url}?colorDisplayCode={color['color_code']}"
                        if color['icon_url']: final_data['thumbnailImageUrl'] = color['icon_url']

                        collected_data.append({
                            "gender": gender, "category": category, 
                            "product_id": final_data['goodsNo'], 
                            "raw_json": json.dumps(final_data, ensure_ascii=False)
                        })
                else:
                    collected_data.append({
                        "gender": gender, "category": category, 
                        "product_id": clean_id, 
                        "raw_json": json.dumps(product_dict, ensure_ascii=False)
                    })
                    print(f"   âœ… {clean_id} ë‹¨ì¼ ì €ì¥ (ê°€ê²©: {product_dict.get('price')}ì›)")
            else:
                print(f"   âš ï¸ {clean_id} ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")

        except Exception as e:
            print(f"   âŒ {clean_id} ì—ëŸ¬: {str(e)[:50]}")
        finally:
            await p_page.close()

async def crawl_category(gender, category_name, target_url, context, collected_data):
    print(f"\n>>> ğŸ¯ [{gender}-{category_name}] ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘")
    page = await context.new_page()
    product_ids = set()
    
    try:
        await page.goto(target_url, timeout=60000, wait_until="domcontentloaded")
        print("   ğŸ“œ ëª©ë¡ ìŠ¤í¬ë¡¤ ì¤‘...")
        for _ in range(5):
            await page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
            await asyncio.sleep(1.0)
        
        content = await page.content()
        matches = re.findall(r"/products/([A-Z0-9-]+)", content)
        for pid in matches:
            if len(pid) >= 5 and "review" not in pid:
                product_ids.add(pid)
        
        print(f"   ğŸ”— ì´ ë°œê²¬ëœ ìƒí’ˆ ìˆ˜: {len(product_ids)}ê°œ")
        await page.close()
        
        tasks = [process_product(pid, gender, category_name, context, collected_data) for pid in list(product_ids)]
        await asyncio.gather(*tasks)

    except Exception as e:
        print(f"   âŒ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        await page.close()

async def run():
    print(f"--- [START] UNIQLO ì•ˆì „ ëª¨ë“œ ìˆ˜ì§‘ ---")
    collected_data = [] 
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True, 
            args=["--start-maximized", "--disable-blink-features=AutomationControlled"]
        ) 
        context = await browser.new_context(
            viewport={"width": 1920, "height": 1080},
            locale="ko-KR",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )
        await context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        for gender, categories in TARGET_MAP.items():
            for category, urls in categories.items():
                if isinstance(urls, str): urls = [urls]
                for url in urls:
                    await crawl_category(gender, category, url, context, collected_data)
        
        await browser.close()

    if collected_data:
        print(f"\nğŸ“¦ {len(collected_data)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ. Spark ì €ì¥ ì‹œì‘...")
        pdf = pd.DataFrame(collected_data)
        
        spark = SparkSession.builder \
            .appName(f"{BRAND_NAME}_Crawler") \
            .config("spark.master", "local[1]") \
            .config("spark.driver.memory", "4g") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .getOrCreate()
        
        try:
            df = spark.createDataFrame(pdf, schema=schema).coalesce(1)
            temp_path = f"crawlers/data/temp_{BRAND_NAME}_output"
            df.write.mode("overwrite").json(temp_path)
            
            if not os.path.exists(LOCAL_OUTPUT_PATH):
                os.makedirs(LOCAL_OUTPUT_PATH)

            json_files = glob.glob(f"{temp_path}/*.json")
            for file in json_files:
                with open(file, 'r', encoding='utf-8') as f:
                    for line in f:
                        row = json.loads(line)
                        raw_data = json.loads(row['raw_json'])
                        
                        filename = f"{BRAND_NAME}_{row['gender'].lower()}_{row['category'].lower()}_{row['product_id']}.json"
                        with open(os.path.join(LOCAL_OUTPUT_PATH, filename), 'w', encoding='utf-8') as out_f:
                            json.dump(raw_data, out_f, ensure_ascii=False, indent=4)
            print(f"\nâœ¨ ì €ì¥ ì™„ë£Œ: {LOCAL_OUTPUT_PATH}")
        finally:
            spark.stop()
    else:
        print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(run())
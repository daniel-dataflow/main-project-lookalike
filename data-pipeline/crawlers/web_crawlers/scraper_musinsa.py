import os
import asyncio
import re
import json
from datetime import datetime
from playwright.async_api import async_playwright
from hdfs import InsecureClient # pip install hdfs í•„ìš”

# --- ì„¤ì • ---
BRAND_NAME = "musinsa"
LOCAL_OUTPUT_PATH = f"crawlers/data/{BRAND_NAME}_json_files"

# --- í•˜ë‘¡ ì„¤ì • ---
HDFS_URL = "http://your-namenode-host:9870"  # ë„¤ì„ë…¸ë“œ IP ë° í¬íŠ¸
HDFS_USER = "hadoop"                         # í•˜ë‘¡ ìœ ì €ëª…
DATE_STR = datetime.now().strftime('%Y%m%d') # ì˜¤ëŠ˜ ë‚ ì§œ (ì˜ˆ: 20240522)
# ì €ì¥ êµ¬ì¡°: raw/brand/date
HDFS_BASE_PATH = f"/user/{HDFS_USER}/raw/{BRAND_NAME}/{DATE_STR}"

# í•˜ë‘¡ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    hdfs_client = InsecureClient(HDFS_URL, user=HDFS_USER)
    # ì‹œì‘ ì „ í•˜ë‘¡ ë””ë ‰í† ë¦¬ ë¯¸ë¦¬ ìƒì„±
    hdfs_client.makedirs(HDFS_BASE_PATH)
except Exception as e:
    print(f"âš ï¸ í•˜ë‘¡ ì—°ê²° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

visited_products = set()
sem = asyncio.Semaphore(4)
async def extract_attribute_focus_data(page):
    try:
        # [1] ê°€ë²¼ìš´ ìŠ¤í¬ë¡¤ (ì†ì„± ì •ë³´ ë¡œë”©ìš©)
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight * 0.4)")
        await asyncio.sleep(0.5)
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight * 0.7)")
        await asyncio.sleep(1.0)

        data = await page.evaluate("""() => {
            const result = {};
            result.scraped_url = location.href;

            // --- A. ê¸°ë³¸ ì •ë³´ ---
            let goodsInfo = {};
            try {
                const script = document.getElementById('__NEXT_DATA__');
                if (script) {
                    const nextData = JSON.parse(script.innerText);
                    const state = nextData.props?.pageProps?.initialState || 
                                  nextData.props?.pageProps?.dehydratedState?.queries?.[0]?.state?.data || {};
                    goodsInfo = state.products?.goods || state.goods || {};
                }
            } catch(e) {}

            result.product_id = goodsInfo.goodsNo || location.href.match(/\d+/)[0];

            // [í•µì‹¬] ìƒí’ˆëª… ê¹”ë”í•˜ê²Œ ì •ì œ
            let rawName = document.querySelector('meta[property="og:title"]')?.content || document.title;
            rawName = rawName.split(' - ')[0].trim(); // ë’·ë¶€ë¶„ ì œê±°
            
            result.product_name = rawName
                .replace(/^\[?ë¬´ì‹ ì‚¬\s?ìŠ¤íƒ ë‹¤ë“œ.*?\]?\s*/, '')   // [ë¬´ì‹ ì‚¬ ìŠ¤íƒ ë‹¤ë“œ] ì œê±°
                .replace(/\(MUSINSA STANDARD\)\s*/i, '')     // (MUSINSA STANDARD) ì œê±°
                .replace(/^\[?ì¿¨íƒ ë‹¤ë“œ.*?\]?\s*/, '')          // [ì¿¨íƒ ë‹¤ë“œ] ì œê±°
                .trim();

            result.product_code = goodsInfo.goodsCode || "";
            if (!result.product_code) {
                // í’ˆë²ˆ í…ìŠ¤íŠ¸ ì°¾ê¸°
                const dt = Array.from(document.querySelectorAll('dt, th, span')).find(e => e.innerText.trim() === 'í’ˆë²ˆ');
                if (dt) {
                    const val = dt.nextElementSibling || dt.parentElement.querySelector('dd');
                    if (val) result.product_code = val.innerText.trim();
                }
            }

            // ê°€ê²©
            let p = goodsInfo.salePrice || goodsInfo.goodsPrice || 0;
            if (typeof p === 'object') p = p.salePrice || 0;
            if (!p || p === 0) {
                 const metaPrice = document.querySelector('meta[property="product:price:amount"]')?.content;
                 if (metaPrice) p = parseInt(metaPrice);
            }
            result.price = p;

            // --- B. ì´ë¯¸ì§€ ---
            const images = [];
            const ogImg = document.querySelector('meta[property="og:image"]')?.content;
            if (ogImg) images.push(ogImg);

            document.querySelectorAll('img').forEach(img => {
                const alt = img.alt || "";
                if (alt.includes('content-img')) {
                    let src = img.getAttribute('data-src') || img.src;
                    if (src && !src.startsWith('data:')) {
                        images.push(src.startsWith('//') ? 'https:' + src : src);
                    }
                }
            });
            result.images = [...new Set(images)];

            // --- C. [í•µì‹¬] í•/ê³„ì ˆê°/ë‘ê»˜ ì¶”ì¶œ (Key-Value ë§¤í•‘ ê°•í™”) ---
            const attributes = {};
            const targetKeys = ['í•', 'ì´‰ê°', 'ì‹ ì¶•ì„±', 'ë¹„ì¹¨', 'ë‘ê»˜', 'ê³„ì ˆ', 'ì•ˆê°'];
            
            // 1. í…Œì´ë¸” í˜•íƒœ (th -> td) ìŠ¤ìº”
            document.querySelectorAll('tr').forEach(tr => {
                const th = tr.querySelector('th');
                const td = tr.querySelector('td');
                if (th && td) {
                    const key = th.innerText.trim();
                    if (targetKeys.includes(key)) {
                        attributes[key] = td.innerText.trim();
                    }
                }
            });

            // 2. ì„ íƒí˜• UI (ë¬´ì‹ ì‚¬ íŠ¹ìœ ì˜ .hausPV í´ë˜ìŠ¤) ìŠ¤ìº”
            // ê° ì„¹ì…˜(ul, div)ì„ ìˆœíšŒí•˜ë©° í—¤ë”ì™€ ì„ íƒëœ ê°’ì„ ì°¾ìŒ
            if (Object.keys(attributes).length === 0) {
                document.querySelectorAll('.MaterialInfo__MaterialWrap-sc-o69dy9-2, ul, div').forEach(container => {
                    const header = container.querySelector('li, span, th, dt');
                    if (header) {
                        const key = header.innerText.trim();
                        if (targetKeys.includes(key)) {
                            // í•´ë‹¹ ì»¨í…Œì´ë„ˆ(Row) ì•ˆì—ì„œ 'hausPV' (ì„ íƒëœ ê°’) ì°¾ê¸°
                            const selected = container.querySelector('.hausPV');
                            if (selected) {
                                attributes[key] = selected.innerText.trim();
                            }
                        }
                    }
                });
            }
            
            // 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë³´ì • (í˜•ì œ ìš”ì†Œ í™•ì¸)
            if (Object.keys(attributes).length === 0) {
                document.querySelectorAll('dt, th, span').forEach(el => {
                     const txt = el.innerText.trim();
                     if (targetKeys.includes(txt) && !attributes[txt]) {
                         const val = el.nextElementSibling;
                         if (val) attributes[txt] = val.innerText.trim();
                     }
                });
            }

            result.attributes = attributes;
            // ì‚¬ì´ì¦ˆ í…Œì´ë¸”ì€ ì œê±°ë¨

            return result;
        }""")
        
        data['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        return data
    except Exception as e:
        return {"error": str(e)}

async def process_product(product_id, gender, category, context):
    if product_id in visited_products: return
    visited_products.add(product_id)
    
    async with sem:
        url = f"https://www.musinsa.com/products/{product_id}"
        p_page = await context.new_page()
        
        # ë¦¬ì†ŒìŠ¤ ìµœì í™”
        await p_page.route("**/*review*", lambda route: route.abort())
        await p_page.route("**/*recommend*", lambda route: route.abort())
        
        try:
            print(f"   ğŸ” {product_id} ë¶„ì„ ì¤‘...")
            await p_page.goto(url, timeout=60000, wait_until="domcontentloaded")
            raw_data = await extract_attribute_focus_data(p_page)
            
            if raw_data and 'product_id' in raw_data:
                raw_data['gender'] = gender
                raw_data['category'] = category
                
                # 1. ë¡œì»¬ì— ì„ì‹œ íŒŒì¼ ìƒì„±
                if not os.path.exists(LOCAL_OUTPUT_PATH): os.makedirs(LOCAL_OUTPUT_PATH)
                filename = f"musinsa_{product_id}.json"
                local_file_path = os.path.join(LOCAL_OUTPUT_PATH, filename)
                
                with open(local_file_path, 'w', encoding='utf-8') as f:
                    json.dump(raw_data, f, ensure_ascii=False, indent=4)
                
                # 2. í•˜ë‘¡ ì—…ë¡œë“œ ë° ë¡œì»¬ ì‚­ì œ
                try:
                    hdfs_full_path = f"{HDFS_BASE_PATH}/{filename}"
                    # overwrite=True: ì´ë¯¸ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
                    hdfs_client.upload(hdfs_full_path, local_file_path, overwrite=True)
                    
                    # ì—…ë¡œë“œ ì„±ê³µ ì‹œ ë¡œì»¬ ì‚­ì œ
                    if os.path.exists(local_file_path):
                        os.remove(local_file_path)
                        save_status = "HDFS ì—…ë¡œë“œ ì™„ë£Œ & ë¡œì»¬ ì‚­ì œ"
                except Exception as he:
                    save_status = f"HDFS ì „ì†¡ ì‹¤íŒ¨: {str(he)[:30]}"
                
                attr_str = ", ".join(list(raw_data.get('attributes', {}).keys()))
                print(f"   âœ… {product_id} | {save_status} | ì†ì„±: [{attr_str}]")
                
        except Exception as e:
            print(f"   âŒ {product_id} ì—ëŸ¬: {str(e)[:50]}")
        finally:
            await p_page.close()

async def crawl_category(gender, category, url, context):
    print(f"\n>>> ğŸ¯ [{gender}-{category}] ëª©ë¡ ê²€ìƒ‰")
    page = await context.new_page()
    product_ids = set()
    try:
        await page.goto(url, timeout=60000)
        for _ in range(3): 
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1.0)
        hrefs = await page.evaluate("""() => Array.from(document.querySelectorAll('a')).map(a => a.href)""")
        for h in hrefs:
            m = re.search(r'(?:goods|products)\/(\d+)', h)
            if m: product_ids.add(m.group(1))
        await page.close()
        await asyncio.gather(*[process_product(pid, gender, category, context) for pid in list(product_ids)])
    except Exception as e:
        print(f"   âŒ ëª©ë¡ ì‹¤íŒ¨: {e}")
        await page.close()

async def run():
    print("--- [START] ë¬´ì‹ ì‚¬ í•/ê³„ì ˆê° ì§‘ì¤‘ í¬ë¡¤ëŸ¬ ---")
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(viewport={"width": 1920, "height": 1080})
        for gender, categories in TARGET_MAP.items():
            for category, urls in categories.items():
                for url in urls: await crawl_category(gender, category, url, context)
        await browser.close()
    print(f"\nâœ¨ ìˆ˜ì§‘ ì¢…ë£Œ")

if __name__ == "__main__":
    asyncio.run(run())
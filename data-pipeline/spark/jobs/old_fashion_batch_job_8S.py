# fashion_batch_job_8S.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, lit, col, current_timestamp, concat, format_string, input_file_name, monotonically_increasing_id, regexp_extract
from pyspark.sql.types import StringType, IntegerType, StructType, StructField
import re
import subprocess
import psycopg2
from bs4 import BeautifulSoup
from datetime import datetime

# --- [1. ì„¤ì • ì •ë³´] ---
MONGO_IP = "localhost"
BRAND_NAME = "8seconds"  # í´ë”ëª… ë° ë¸Œëœë“œ êµ¬ë¶„ìš©
BRAND_PREFIX = "8S"      # 8seconds ì „ìš© ID ì ‘ë‘ì‚¬
TARGET_DATE = datetime.now().strftime("%Y%m%d") # ì‹¤ì œ ìš´ì˜ì‹œ ì‚¬ìš©
#TARGET_DATE = "20260205" 

HDFS_BASE = "hdfs://localhost:9000"
RAW_PATH = f"/raw/{BRAND_NAME}/{TARGET_DATE}"
IMAGE_DIR = f"{RAW_PATH}/image"
CONTAINER_NAME = "namenode-main"

spark = SparkSession.builder \
    .appName(f"{BRAND_NAME}_ETL_{TARGET_DATE}") \
    .config("spark.mongodb.write.connection.uri", f"mongodb://datauser:DataPass2024!@{MONGO_IP}:27017/datadb.product_details?authSource=admin") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1") \
    .getOrCreate()

# --- [2. ID ì±„ë²ˆ í•¨ìˆ˜] ---
def get_and_update_sequence(count):
    conn = psycopg2.connect(host="localhost", database="datadb", user="datauser", password="DataPass2024!")
    cur = conn.cursor()
    cur.execute("""
        UPDATE brand_sequences 
        SET last_seq = last_seq + %s 
        WHERE brand_name = %s 
        RETURNING last_seq - %s + 1
    """, (count, BRAND_NAME.upper(), count))
    result = cur.fetchone()
    if not result:
        # 8secondsê°€ í…Œì´ë¸”ì— ì—†ì„ ê²½ìš° ì´ˆê¸°ê°’ ì„¸íŒ…
        cur.execute("INSERT INTO brand_sequences (brand_name, last_seq) VALUES (%s, %s) RETURNING 1", (BRAND_NAME.upper(), count))
        start_num = 1
    else:
        start_num = result[0]
    conn.commit()
    cur.close()
    conn.close()
    return start_num

# --- [3. 8seconds ë§ì¶¤í˜• íŒŒì‹± UDF] ---

def parse_8s_details(html_content, file_path):
    if not html_content or len(html_content) < 100: 
        return "unknown", "unknown", 0, "ë‚´ìš©ì—†ìŒ", None
    
    # 1. ëª¨ë¸ ì½”ë“œ (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ)
    model_code = "unknown"
    try:
        file_name = file_path.split('/')[-1]
        model_code = file_name.split('_')[3] # 4ë²ˆì§¸ ë©ì–´ë¦¬
    except:
        pass
    model_match = re.search(r"GM\d+", file_path)
    if model_match:
        model_code = model_match.group(0)
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 2. ìƒí’ˆëª… (ì „ë‹¬í•´ì£¼ì‹  div class="gods-name" ì¶”ì¶œ)
    name_tag = soup.find("div", class_="gods-name")
    if not name_tag:
        name_tag = soup.find("div", id="goodDtlTitle")
    prod_name = name_tag.get_text(strip=True) if name_tag else "unknown"

    # 3. ê°€ê²© (class="price" ì¶”ì¶œ ë° ì½¤ë§ˆ ì œê±°)
    base_price = 0
    price_tag = soup.select_one(".price-info .price")
    if price_tag:
        try:
            # "99,900" -> "99900" -> 99900 ë³€í™˜
            price_text = re.sub(r"[^\d]", "", price_tag.get_text())
            base_price = int(price_text)
        except:
            base_price = 0

    # ë°±ì—…ìš© ê°€ê²© ì…€ë ‰í„° (SSFìƒµ ê¸°ì¤€)
    if base_price == 0:
        price_tag = soup.select_one(".price em") or \
                    soup.select_one(".sale_price") or \
                    soup.select_one(".discount_price")
        if price_tag:
            base_price = int(re.sub(r"[^\d]", "", price_tag.get_text()))

    # 4. ìƒì„¸ ì„¤ëª…
    desc_tag = soup.find("meta", property="og:description")
    detail_desc = desc_tag['content'] if desc_tag else "ìƒì„¸ ì„¤ëª… ì—†ìŒ"

    # 5. ì´ë¯¸ì§€ URL
    img_tag = soup.find("meta", property="og:image")
    img_url = img_tag['content'] if img_tag else None

    return model_code, prod_name, base_price, detail_desc, img_url


#######################
info_schema = StructType([
    StructField("model_code", StringType(), True),
    StructField("prod_name", StringType(), True),
    StructField("base_price", IntegerType(), True),
    StructField("detail_desc", StringType(), True),
    StructField("img_url", StringType(), True)
])
parse_udf = udf(parse_8s_details, info_schema)

# --- [4. ETL ë¡œì§] ---

# 1. HDFSì—ì„œ 8seconds HTML ì½ê¸°
input_path = f"{HDFS_BASE}{RAW_PATH}/*.html"
print(f"ğŸ“‚ Reading from: {input_path}")

raw_df = spark.read.text(input_path, wholetext=True) \
              .select(input_file_name().alias("file_path"), col("value").alias("html_content"))

# 2. íŒŒì‹± ìˆ˜í–‰
parsed_df = raw_df.withColumn("info", parse_udf(col("html_content"), col("file_path")))

# âš ï¸ ì´ ë¶€ë¶„ì— ìˆë˜ final_df.cache()ëŠ” ì•„ì§ ë³€ìˆ˜ê°€ ìƒì„± ì „ì´ë¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.
parsed_df.cache() # ì°¨ë¼ë¦¬ parsed_dfë¥¼ ìºì‹œí•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

total_count = parsed_df.count()
if total_count == 0:
    print(f"âŒ {BRAND_NAME} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    spark.stop()
    exit()

# [ìˆ˜ì • 2] ì‹œí€€ìŠ¤ ë²ˆí˜¸ë¥¼ ë¨¼ì € ë°›ì•„ì˜µë‹ˆë‹¤ (ê·¸ë˜ì•¼ product_idë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ)
start_seq = get_and_update_sequence(total_count)

# ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„±
subprocess.run(f"docker exec {CONTAINER_NAME} hdfs dfs -mkdir -p {IMAGE_DIR}", shell=True)

## [ìˆ˜ì • 3] final_dfë¥¼ í•œ ë²ˆë§Œ, ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì •ì˜í•©ë‹ˆë‹¤.
# ì„±ë³„(gender) + ì¹´í…Œê³ ë¦¬(sub_category)ë¥¼ í•©ì³ì„œ Men_Top í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.
# ëª¨ë“  .withColumnì´ ìˆ˜ì§ìœ¼ë¡œ ì˜ˆì˜ê²Œ ì •ë ¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
final_df = parsed_df.withColumn("idx", monotonically_increasing_id() + 1) \
    .withColumn("product_id", format_string(f"{BRAND_PREFIX}%04d", col("idx") + start_seq - 1)) \
    .withColumn("img_hdfs_path", concat(lit(IMAGE_DIR), lit(f"/{BRAND_NAME}_"), col("info.model_code"), lit(".jpg"))) \
    .withColumn("gender", regexp_extract(col("file_path"), r"8seconds_([A-Za-z]+)_", 1)) \
    .withColumn("sub_category", regexp_extract(col("file_path"), r"8seconds_[A-Za-z]+_([A-Za-z]+)_", 1)) \
    .withColumn("category_code", concat(col("gender"), lit("_"), col("sub_category")))

final_df.cache()

# --- [5. PostgreSQL ì ì¬] ---
pg_data = final_df.select(
    col("product_id"),
    col("info.model_code").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("info.prod_name").alias("prod_name"),
    col("info.base_price").alias("base_price"),
    col("category_code"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt"),
    current_timestamp().alias("update_dt")
)

pg_data.write.format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/datadb").option("driver", "org.postgresql.Driver") \
    .option("dbtable", "products").option("user", "datauser").option("password", "DataPass2024!") \
    .mode("append").save()

# --- [6. MongoDB ì ì¬] ---
mongo_data = final_df.select(
    col("product_id"), col("info.model_code").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("info.detail_desc").alias("detail_desc"),
    col("img_hdfs_path"), current_timestamp().alias("create_dt")
)

mongo_data.write.format("mongodb").mode("append").option("database", "datadb").option("collection", "product_details").save()

# --- [7. ì´ë¯¸ì§€ ì²˜ë¦¬] ---
image_list = final_df.select("info.img_url", "info.model_code").collect()
for row in image_list:
    if row.img_url and row.model_code != "unknown":
        hdfs_target_path = f"{IMAGE_DIR}/{row.model_code}.jpg"
        cmd = f"wget -qO- {row.img_url} | docker exec -i {CONTAINER_NAME} hdfs dfs -put - {hdfs_target_path}"
        subprocess.run(cmd, shell=True)

print(f"âœ… {BRAND_NAME.upper()} {total_count}ê±´ ì ì¬ ë° ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ!")
spark.stop()
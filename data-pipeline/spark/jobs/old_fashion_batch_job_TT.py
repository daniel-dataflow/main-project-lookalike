# fashion_batch_job_TT.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, lit, col, current_timestamp, concat, format_string, input_file_name, monotonically_increasing_id, regexp_extract
from pyspark.sql.types import StringType, IntegerType, StructType, StructField
import re
import subprocess
import psycopg2
import json
from bs4 import BeautifulSoup
from datetime import datetime

# --- [1. ì„¤ì • ì •ë³´] ---
MONGO_IP = "localhost"
BRAND_NAME = "topten"      # HDFS ê²½ë¡œ ë° ë¸Œëœë“œ êµ¬ë¶„
BRAND_PREFIX = "TT"        # íƒ‘í… ì „ìš© ID ì ‘ë‘ì‚¬ (TopTen)
TARGET_DATE = datetime.now().strftime("%Y%m%d") # ìˆ˜ì§‘ ë‚ ì§œ (í•„ìš”ì‹œ ìˆ˜ë™ ì§€ì •)
#TARGET_DATE = "20260205"



HDFS_BASE = "hdfs://localhost:9000"
# í¬ë¡¤ëŸ¬ ì†ŒìŠ¤ì½”ë“œì˜ HDFS_BASE_PATHì¸ /datalake/raw/topten êµ¬ì¡°ë¥¼ ë°˜ì˜
#RAW_PATH = f"/raw/{BRAND_NAME}/{TARGET_DATE}"
RAW_PATH = f"/raw/topten/{TARGET_DATE}"
IMAGE_DIR = f"{RAW_PATH}/image"
CONTAINER_NAME = "namenode-main"  # <--- docker psì—ì„œ í™•ì¸ëœ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •

spark = SparkSession.builder \
    .appName(f"{BRAND_NAME}_ETL_{TARGET_DATE}") \
    .config("spark.mongodb.write.connection.uri", f"mongodb://datauser:DataPass2024!@{MONGO_IP}:27017/datadb.product_details?authSource=admin") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1") \
    .getOrCreate()

# --- [2. ID ì±„ë²ˆ í•¨ìˆ˜] ---
def get_and_update_sequence(count):
    conn = psycopg2.connect(host="localhost", database="datadb", user="datauser", password="DataPass2024!")
    cur = conn.cursor()
    cur.execute("""
        UPDATE brand_sequences 
        SET last_seq = last_seq + %s 
        WHERE brand_name = %s 
        RETURNING last_seq - %s + 1
    """, (count, BRAND_NAME.upper(), count))
    result = cur.fetchone()
    if not result:
        cur.execute("INSERT INTO brand_sequences (brand_name, last_seq) VALUES (%s, %s) RETURNING 1", (BRAND_NAME.upper(), count))
        start_num = 1
    else:
        start_num = result[0]
    conn.commit()
    cur.close()
    conn.close()
    return start_num

# --- [3. TOPTEN10 ë§ì¶¤í˜• íŒŒì‹± UDF] ---

def parse_tt_details(html_content, file_path):
    if not html_content or len(html_content) < 500: 
        return "unknown", "unknown", 0, "ë‚´ìš©ì—†ìŒ", None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 1. ëª¨ë¸ ì½”ë“œ (íŒŒì¼ëª… ê¸°ë°˜)
    model_code = "unknown"
    try:
        model_code = file_path.split('/')[-1].split('_')[3]
    except: pass

    # ê¸°ë³¸ê°’ ì„¤ì •
    prod_name = "unknown"
    base_price = 0
    img_url = None

    # --- [í•µì‹¬: JSON-LD ë°ì´í„° íŒŒì‹±] ---
    try:
        # <script type="application/ld+json"> íƒœê·¸ë¥¼ ëª¨ë‘ ì°¾ìŒ
        scripts = soup.find_all("script", type="application/ld+json")
        for script in scripts:
            data = json.loads(script.string)
            # @typeì´ Productì¸ JSON ë°ì´í„°ë¥¼ ì°¾ìŒ
            if data.get("@type") == "Product":
                prod_name = data.get("name", prod_name)
                img_url = data.get("image", img_url)
                # offers ì•ˆì— ìˆëŠ” price ì¶”ì¶œ
                offers = data.get("offers", {})
                if isinstance(offers, dict):
                    price_val = offers.get("price")
                    if price_val:
                        base_price = int(float(str(price_val)))
                break
    except Exception as e:
        print(f"JSON íŒŒì‹± ì—ëŸ¬: {e}")

    # --- [ë°±ì—…: JSON ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹] ---
    if prod_name == "unknown":
        og_title = soup.find("meta", property="og:title")
        prod_name = og_title['content'].split('|')[0].strip() if og_title else "unknown"
    
    if base_price == 0:
        price_tag = soup.select_one("#salePrice") or soup.select_one(".gods-price .sale")
        if price_tag:
            base_price = int(re.sub(r"[^\d]", "", price_tag.get_text()))

    if not img_url:
        og_img = soup.find("meta", property="og:image")
        img_url = og_img['content'] if og_img else None

    return model_code, prod_name, base_price, "ìƒì„¸ì„¤ëª…", img_url


#################
info_schema = StructType([
    StructField("model_code", StringType(), True),
    StructField("prod_name", StringType(), True),
    StructField("base_price", IntegerType(), True),
    StructField("detail_desc", StringType(), True),
    StructField("img_url", StringType(), True)
])
parse_udf = udf(parse_tt_details, info_schema)

# --- [4. ETL ë¡œì§] ---

# HDFS ì£¼ì†Œë¥¼ ëª…í™•íˆ ë¶™ì—¬ì¤ë‹ˆë‹¤.
input_path = f"hdfs://localhost:9000{RAW_PATH}/*.html"
print(f"ğŸ“‚ Reading HTML files from: {input_path}")

raw_df = spark.read.text(input_path, wholetext=True) \
    .withColumnRenamed("value", "html_content") \
    .withColumn("file_path", input_file_name())

print(f"ğŸ“Š ì½ì–´ì˜¨ íŒŒì¼ ê°œìˆ˜: {raw_df.count()}ê°œ")

# ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§ ë° íŒŒì‹±
parsed_df = raw_df.withColumn("info", parse_udf(col("html_content"), col("file_path")))
parsed_df = parsed_df.filter(col("info.prod_name") != "")
parsed_df.select("file_path", "info.prod_name", "info.base_price").show(5, truncate=False)

parsed_df.cache()

total_count = parsed_df.count()
if total_count == 0:
    print(f"âŒ {BRAND_NAME.upper()} ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    spark.stop()
    exit()

start_seq = get_and_update_sequence(total_count)

# ìµœì¢… ë°ì´í„° ê°€ê³µ
# íŒŒì¼ëª…ì—ì„œ genderì™€ categoryë¥¼ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
# ì˜ˆ: topten_Men_Outer_... -> gender: Men, category: Outer
final_df = parsed_df.withColumn("idx", monotonically_increasing_id() + 1) \
    .withColumn("product_id", format_string(f"{BRAND_PREFIX}%04d", col("idx") + start_seq - 1)) \
    .withColumn("img_hdfs_path", concat(lit(IMAGE_DIR), lit(f"/{BRAND_NAME}_"), col("info.model_code"), lit(".jpg"))) \
    .withColumn("gender", regexp_extract(col("file_path"), r"topten_([A-Za-z]+)_", 1)) \
    .withColumn("sub_category", regexp_extract(col("file_path"), r"topten_[A-Za-z]+_([A-Za-z]+)_", 1)) \
    .withColumn("category_code", concat(col("gender"), lit("_"), col("sub_category")))

# --- [5. PostgreSQL ì ì¬] ---
pg_data = final_df.select(
    col("product_id"),
    col("info.model_code").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("info.prod_name").alias("prod_name"),
    col("info.base_price").alias("base_price"),
    col("category_code"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt"),
    current_timestamp().alias("update_dt")
)

pg_data.write.format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/datadb").option("driver", "org.postgresql.Driver") \
    .option("dbtable", "products").option("user", "datauser").option("password", "DataPass2024!") \
    .mode("append").save()

# --- [6. MongoDB ì ì¬] ---
mongo_data = final_df.select(
    col("product_id"), col("info.model_code").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("info.detail_desc").alias("detail_desc"),
    col("img_hdfs_path"), current_timestamp().alias("create_dt")
)

mongo_data.write.format("mongodb").mode("append").option("database", "datadb").option("collection", "product_details").save()

# --- [7. ì´ë¯¸ì§€ ì²˜ë¦¬ (ì¤‘ë³µ ì—…ë¡œë“œ ë°©ì§€ ë¡œì§ í¬í•¨)] ---
# ì´ë¯¸ í¬ë¡¤ëŸ¬ì—ì„œ ì´ë¯¸ì§€ë¥¼ HDFSì— ì˜¬ë ¸ë‹¤ë©´ ì´ ê³¼ì •ì€ ìƒëµ ê°€ëŠ¥í•˜ì§€ë§Œ, 
# ë§Œì•½ í¬ë¡¤ëŸ¬ì—ì„œ ëˆ„ë½ëœ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ë³´ì¶©í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
# selectë¬¸ì— "img_hdfs_path"ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
image_list = final_df.select("info.img_url", "info.model_code", "img_hdfs_path").collect()

for row in image_list:
    # ì´ë¯¸ì§€ URLì´ ìˆê³  ëª¨ë¸ ì½”ë“œê°€ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    if row.img_url and row.model_code != "unknown":
        # 1. HDFSì— íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬ (ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
        check_cmd = f"docker exec {CONTAINER_NAME} hdfs dfs -test -e {row.img_hdfs_path}"
        exists = subprocess.run(check_cmd, shell=True).returncode
        
        if exists != 0: # íŒŒì¼ì´ ì—†ìœ¼ë©´(return codeê°€ 0ì´ ì•„ë‹ˆë©´) ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
            # 2. wgetìœ¼ë¡œ ë‹¤ìš´ë°›ì•„ ë°”ë¡œ dockerë¥¼ í†µí•´ HDFSë¡œ ìŠ¤íŠ¸ë¦¬ë° ì €ì¥
            # row.img_hdfs_path ì˜ˆ: /raw/topten/20260205/image/topten_MSF4VP1502NVP.jpg
            cmd = f"wget -qO- {row.img_url} | docker exec -i {CONTAINER_NAME} hdfs dfs -put - {row.img_hdfs_path}"
            subprocess.run(cmd, shell=True)

print(f"âœ… {BRAND_NAME.upper()} {total_count}ê±´ ì ì¬ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ!")
spark.stop()
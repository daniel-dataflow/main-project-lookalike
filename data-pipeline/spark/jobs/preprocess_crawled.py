from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, lit, col, struct, array
from pyspark.sql.types import StringType, ArrayType, IntegerType, StructType, StructField
import os
import re
import uuid
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# 1. Spark ì„¸ì…˜ ì„¤ì •
spark = SparkSession.builder \
    .appName("Lookalike_Daily_ETL_Full_Pipeline") \
    .config("spark.mongodb.write.connection.uri", "mongodb://127.0.0.1:27017/lookalike.product_details") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1") \
    .getOrCreate()

# 2. ë‚ ì§œ ë° ê²½ë¡œ ì„¤ì •
target_date = datetime.now().strftime("%Y%m%d")
raw_html_path = f"hdfs://localhost:9000/lookalike/raw/*/{target_date}/*.html"

# --- [3. UDF ë° í•¨ìˆ˜ ì •ì˜ ì˜ì—­: ì‹¤í–‰ ì „ ëª¨ë‘ ì •ì˜ë˜ì–´ì•¼ í•¨] ---

# ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
def classify_category(file_path):
    file_name = file_path.split('/')[-1].lower()
    gender = "woman" if "woman" in file_name or "women" in file_name else "man" if "man" in file_name or "men" in file_name else "unknown"
    
    category_type = "etc"
    if any(k in file_name for k in ["outer", "jacket", "coat"]): category_type = "outer"
    elif any(k in file_name for k in ["pants", "bottom", "jeans"]): category_type = "pants"
    elif any(k in file_name for k in ["shirt", "top", "blouse", "t-shirt"]): category_type = "shirt"
    
    return f"{category_type}_{gender}" if gender != "unknown" and category_type != "etc" else category_type

classify_udf = udf(classify_category, StringType())

# ë¸Œëœë“œ ì¶”ì¶œ
def extract_brand(path):
    parts = path.split('/')
    try: return parts[parts.index('raw') + 1]
    except: return "unknown"

brand_udf = udf(extract_brand, StringType())

# HTML ìƒì„¸ ì •ë³´ íŒŒì‹±
def extract_info_from_html(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    name_tag = soup.find(['h1', 'h2']) or soup.select_one('.prod_name, .title')
    prod_name = name_tag.get_text(strip=True) if name_tag else "unknown_product"
    
    price_tag = soup.select_one('.price, .sale_price, .amount')
    price_text = price_tag.get_text(strip=True) if price_tag else "0"
    base_price = int(re.sub(r'[^0-9]', '', price_text)) if price_text != "0" else 0
    
    model_tag = soup.find('meta', {'property': 'product:item_id'})
    model_code = model_tag['content'] if model_tag else str(uuid.uuid4())[:10]
    
    return model_code, prod_name, base_price

info_schema = StructType([
    StructField("model_code", StringType(), True),
    StructField("prod_name", StringType(), True),
    StructField("base_price", IntegerType(), True)
])
info_udf = udf(extract_info_from_html, info_schema)

# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° HDFS ì €ì¥

###
import subprocess
import requests
import uuid
from bs4 import BeautifulSoup
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType

HDFS_BIN = ["docker", "exec", "-i", "namenode-main", "hdfs"]

def extract_and_save_images(html_content, file_path, target_date="20260205"):
    # BeautifulSoup íŒŒì‹±
    soup = BeautifulSoup(html_content, 'html.parser')
    image_urls = [img.get('src') for img in soup.find_all('img', src=True) if img.get('src')]
    
    downloaded_paths = []
    # brand_name ì¶”ì¶œ ë¡œì§ (í•¨ìˆ˜ ë‚´ë¶€ì— ìˆê±°ë‚˜ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ì•¼ í•¨)
    brand_name = file_path.split('/')[-3] # ì˜ˆ: /lookalike/raw/zara/... ì—ì„œ zara ì¶”ì¶œ
    hdfs_image_dir = f"/lookalike/raw/{brand_name}/{target_date}/image/"

    # HDFS ë””ë ‰í† ë¦¬ ë¨¼ì € ìƒì„± (ì—†ì„ ê²½ìš° ëŒ€ë¹„)
    subprocess.run(HDFS_BIN + ["dfs", "-mkdir", "-p", hdfs_image_dir])

    for img_url in list(set(image_urls)):
        if img_url.startswith('//'): img_url = 'https:' + img_url
        if not img_url.startswith('http'): continue # ì˜ëª»ëœ URL ìŠ¤í‚µ
            
        try:
            resp = requests.get(img_url, stream=True, timeout=5)
            resp.raise_for_status()
            
            ext = img_url.split('.')[-1].split('?')[0][:3]
            if len(ext) > 3 or not ext: ext = 'jpg'
            
            file_name = f"{brand_name}_{uuid.uuid4()}.{ext}"
            full_hdfs_path = hdfs_image_dir + file_name
            
            # íŒŒì´í”„ë¥¼ ì´ìš©í•´ HDFSì— ë°”ë¡œ ì“°ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            # hdfs dfs -put - <target_path> ëŠ” í‘œì¤€ ì…ë ¥ì„ ë°›ì•„ HDFS íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
            process = subprocess.Popen(
                HDFS_BIN + ["dfs", "-put", "-", full_hdfs_path], 
                stdin=subprocess.PIPE
            )
            
            for chunk in resp.iter_content(chunk_size=8192):
                if chunk:
                    process.stdin.write(chunk)
            
            process.stdin.close()
            process.wait()
            
            if process.returncode == 0:
                downloaded_paths.append(full_hdfs_path)
                
        except Exception as e:
            continue
            
    return downloaded_paths

# UDF ë“±ë¡ ì‹œ target_date ë“± ì™¸ë¶€ ë³€ìˆ˜ê°€ í•„ìš”í•˜ë©´ lambdaë¥¼ í™œìš©í•˜ì„¸ìš”.
images_udf = udf(lambda h, f: extract_and_save_images(h, f, target_date), ArrayType(StringType()))
###

# --- [4. ETL í”„ë¡œì„¸ìŠ¤ ì‹œì‘] ---

print(f"--- {target_date} íŒŒì´í”„ë¼ì¸ ê°€ë™ ---")
raw_rdd = spark.sparkContext.wholeTextFiles(raw_html_path)
df = raw_rdd.toDF(["file_path", "html_content"])

# ê³µí†µ ê°€ê³µ ë°ì´í„°í”„ë ˆì„ ìƒì„±
processed_df = df.withColumn("brand_name", brand_udf("file_path")) \
                 .withColumn("category_code", classify_udf("file_path")) \
                 .withColumn("process_date", lit(target_date)) \
                 .withColumn("info", info_udf("html_content")) \
                 .withColumn("image_paths", images_udf("html_content", "file_path"))

processed_df.cache()

# 5. PostgreSQL ì ì¬ìš© ë³€í™˜
print("ğŸ˜ PostgreSQL ì ì¬ ì¤‘...")

pg_ready_df = processed_df.select(
    col("info.model_code").alias("model_code"),
    col("brand_name").alias("brand_name"),
    col("info.prod_name").alias("prod_name"),
    col("info.base_price").cast("int").alias("base_price"),
    col("category_code").alias("category_code"),
    # ì—¬ê¸°ë¥¼ img_hdfs_pathì—ì„œ main_img_pathë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤!
    col("image_paths")[0].alias("main_img_path")
)

# PostgreSQL ì €ì¥
pg_ready_df.write \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/datadb") \
    .option("driver", "org.postgresql.Driver") \
    .option("dbtable", "products") \
    .option("user", "datauser") \
    .option("password", "DataPass2024!") \
    .mode("append").save()

print("âœ… PostgreSQL ì ì¬ ì™„ë£Œ!")
# 6. PostgreSQLì—ì„œ ìƒì„±ëœ product_id ê°€ì ¸ì˜¤ê¸° (ì—°ë™ í•µì‹¬)
print("ğŸ”— PostgreSQLì—ì„œ ìƒì„±ëœ ID ë§¤í•‘ ì¤‘...")
# DBì—ì„œ ë°©ê¸ˆ ë“¤ì–´ê°„ IDì™€ model_codeë¥¼ ê°€ì ¸ì˜´
pg_ids_df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/datadb") \
    .option("driver", "org.postgresql.Driver") \
    .option("dbtable", "products") \
    .option("user", "datauser") \
    .option("password", "DataPass2024!") \
    .load() \
    .select(col("product_id"), col("model_code").cast("string").alias("db_model_code"))

# 7. MongoDB ì ì¬ìš© ë°ì´í„° ì¡°ì¸ ë° ë³€í™˜
print("ğŸƒ MongoDB ì ì¬ ì¤‘ (product_details) ...")
from pyspark.sql.functions import current_timestamp, array

# ì›ë³¸ ê°€ê³µ ë°ì´í„°ì™€ DB ID ì¡°ì¸
mongo_ready_df = processed_df.join(
    pg_ids_df, 
    processed_df.info.model_code == pg_ids_df.db_model_code, 
    "inner"
).select(
    col("product_id"),                             # PostgreSQLì˜ PKì™€ ì—°ë™
    col("info.model_code").alias("model_code"),
    col("brand_name").alias("brand_name"),
    col("image_paths").alias("img_hdfs_path"),    # MongoDBëŠ” array íƒ€ì…
    col("html_content").alias("raw_html"),
    lit(None).cast("string").alias("detail_desc"), 
    array().cast("array<string>").alias("keywords"), 
    current_timestamp().alias("create_dt")        # ì…ë ¥ ì‹œê°
)

# MongoDB ì €ì¥
mongo_ready_df.write \
    .format("mongodb") \
    .option("spark.mongodb.write.connection.uri", "mongodb://127.0.0.1:27017/lookalike.product_details") \
    .mode("append").save()

print(f"ğŸš€ {target_date} ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
spark.stop()
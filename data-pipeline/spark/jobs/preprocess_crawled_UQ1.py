from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, lit, col, current_timestamp, concat, format_string, input_file_name, monotonically_increasing_id, regexp_extract, when
from pyspark.sql.types import StringType, IntegerType, StructType, StructField
import re
import subprocess
import psycopg2
from bs4 import BeautifulSoup
from datetime import datetime

# --- [1. ì„¤ì • ì •ë³´] ---
MONGO_IP = "localhost"
BRAND_NAME = "uniqlo"  # ì†Œë¬¸ìë¡œ í´ë”ëª…ê³¼ ì¼ì¹˜ ê¶Œì¥
BRAND_PREFIX = "UQ"
# TARGET_DATE = datetime.now().strftime("%Y%m%d") # ì‹¤ì œ ìš´ì˜ì‹œ ì‚¬ìš©
TARGET_DATE = "20260205" 

# HDFS ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
HDFS_BASE = "hdfs://localhost:9000"
# ìš”ì²­í•˜ì‹  ê²½ë¡œ ê·œì¹™: /raw/ë¸Œëœë“œëª…/ì¼ì/
RAW_PATH = f"/raw/{BRAND_NAME}/{TARGET_DATE}"
input_path = f"{HDFS_BASE}{RAW_PATH}/*.html"
IMAGE_DIR = f"{RAW_PATH}/image"

spark = SparkSession.builder \
    .appName(f"{BRAND_NAME}_ETL_{TARGET_DATE}") \
    .config("spark.mongodb.write.connection.uri", f"mongodb://datauser:DataPass2024!@{MONGO_IP}:27017/datadb.product_details?authSource=admin") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1") \
    .getOrCreate()

# --- [2. ID ì±„ë²ˆ í•¨ìˆ˜] ---
def get_and_update_sequence(count):
    conn = psycopg2.connect(host="localhost", database="datadb", user="datauser", password="DataPass2024!")
    cur = conn.cursor()
    # DBì—ëŠ” ëŒ€ë¬¸ìë¡œ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ UPPER ì‚¬ìš©
    cur.execute("""
        UPDATE brand_sequences 
        SET last_seq = last_seq + %s 
        WHERE brand_name = %s 
        RETURNING last_seq - %s + 1
    """, (count, BRAND_NAME.upper(), count))
    result = cur.fetchone()
    if not result:
        raise Exception(f"ë¸Œëœë“œ {BRAND_NAME}ë¥¼ brand_sequences í…Œì´ë¸”ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    start_num = result[0]
    conn.commit()
    cur.close()
    conn.close()
    return start_num

# --- [3. íŒŒì‹± UDF] ---
def parse_details(html_content):
    if not html_content: return "unknown", "unknown", 0, "ë‚´ìš©ì—†ìŒ", None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ëª¨ë¸ ì½”ë“œ (ì œí’ˆ ë²ˆí˜¸ ì¶”ì¶œ)
    model_text = soup.find(string=re.compile("ì œí’ˆ ë²ˆí˜¸"))
    model_code = re.search(r"(\d+)", model_text).group(1) if model_text else "unknown"

    # ì´ë¯¸ì§€ URL (ì›ë³¸ ì‚¬ì´íŠ¸ ì£¼ì†Œ)
    img_tag = soup.find("meta", property="og:image")
    img_url = img_tag['content'] if img_tag else None

    # ìƒí’ˆëª… ë° ê°€ê²©
    meta_title = soup.find("meta", property="og:title")
    prod_name = meta_title['content'].replace("| UNIQLO KR", "").strip() if meta_title else "unknown"
    
    price_tag = soup.select_one(".fr-ec-price")
    base_price = int(re.sub(r'[^0-9]', '', price_tag.get_text())) if price_tag else 0

    # ìƒì„¸ ì„¤ëª…
    desc_elements = soup.select(".image-plus-text__horizontal-large-description, [data-testid='pdp-description-area']")
    detail_desc = "\n".join([d.get_text(strip=True) for d in desc_elements]) if desc_elements else "ìƒì„¸ ì„¤ëª… ì—†ìŒ"

    return model_code, prod_name, base_price, detail_desc, img_url

info_schema = StructType([
    StructField("model_code", StringType(), True),
    StructField("prod_name", StringType(), True),
    StructField("base_price", IntegerType(), True),
    StructField("detail_desc", StringType(), True),
    StructField("img_url", StringType(), True)
])
parse_udf = udf(parse_details, info_schema)

# --- [4. ETL ë¡œì§] ---

# 1. ë™ì  ê²½ë¡œì—ì„œ HTML ì½ê¸°
input_path = f"{HDFS_BASE}{RAW_PATH}/*.html"
print(f"ğŸ“‚ Reading from: {input_path}")

raw_df = spark.read.text(input_path, wholetext=True) \
              .select(input_file_name().alias("file_path"), col("value").alias("html_content"))

# 2. íŒŒì‹± ë° ê¸°ë³¸ ê°€ê³µ
parsed_df = raw_df.withColumn("info", parse_udf(col("html_content")))

total_count = parsed_df.count()
if total_count == 0:
    print(f"âŒ {TARGET_DATE} ì¼ìì— ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    spark.stop()
    exit()

# 3. ID ì±„ë²ˆ ë° ê²½ë¡œ ìƒì„±
start_seq = get_and_update_sequence(total_count)

# HDFS ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ìƒì„± (Docker ëª…ë ¹ì–´ ì‚¬ìš©)
CONTAINER_NAME = "namenode-main"
subprocess.run(f"docker exec {CONTAINER_NAME} hdfs dfs -mkdir -p {IMAGE_DIR}", shell=True)

final_df = parsed_df.withColumn("idx", monotonically_increasing_id() + 1) \
    .withColumn("product_id", format_string(f"{BRAND_PREFIX}%04d", col("idx") + start_seq - 1)) \
    .withColumn("img_hdfs_path", concat(lit(IMAGE_DIR), lit("/"), col("info.model_code"), lit(".jpg"))) \
    .withColumn("category_code", lit("outer")) # í•„ìš”ì‹œ ê²½ë¡œì—ì„œ ì¶”ì¶œ ë¡œì§ ì¶”ê°€

final_df.cache()

# --- [5. PostgreSQL ì ì¬] ---
pg_data = final_df.select(
    col("product_id"),
    col("info.model_code").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("info.prod_name").alias("prod_name"),
    col("info.base_price").alias("base_price"),
    col("category_code"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt"),
    current_timestamp().alias("update_dt")
)

pg_data.write.format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/datadb") \
    .option("driver", "org.postgresql.Driver") \
    .option("dbtable", "products") \
    .option("user", "datauser") \
    .option("password", "DataPass2024!") \
    .mode("append").save()

# --- [6. MongoDB ì ì¬] ---
mongo_data = final_df.select(
    col("product_id"),
    col("info.model_code").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("info.detail_desc").alias("detail_desc"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt")
)

mongo_data.write.format("mongodb") \
    .mode("append") \
    .option("database", "datadb") \
    .option("collection", "product_details") \
    .save()

# --- [7. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° HDFS ì—…ë¡œë“œ] ---
image_list = final_df.select("info.img_url", "info.model_code").collect()

for row in image_list:
    if row.img_url and row.model_code != "unknown":
        img_filename = f"{row.model_code}.jpg"
        hdfs_target_path = f"{IMAGE_DIR}/{img_filename}"
        
        # ì›ë³¸ URLì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì•„ ë°”ë¡œ HDFSë¡œ íŒŒì´í”„ ì—°ê²°
        cmd = f"wget -qO- {row.img_url} | docker exec -i {CONTAINER_NAME} hdfs dfs -put - {hdfs_target_path}"
        subprocess.run(cmd, shell=True)

print(f"âœ… {BRAND_NAME.upper()} {total_count}ê±´ ì ì¬ ë° ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ! (Path: {IMAGE_DIR})")
spark.stop()
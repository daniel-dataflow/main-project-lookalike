# fashion_batch_job_TT.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import (lit, col, current_timestamp, concat, format_string, 
                                   input_file_name, regexp_extract, lower, element_at, 
                                   coalesce, to_json, row_number, udf)
from pyspark.sql.window import Window
from pyspark.sql.types import StringType
import json
import subprocess
import psycopg2
from datetime import datetime

# --- [1. ì„¤ì • ì •ë³´] ---
BRAND_NAME = "topten"  # ì†Œë¬¸ìë¡œ í†µì¼ (HDFS ê²½ë¡œìš©)
BRAND_PREFIX = "TT"    # Toptenì˜ ì•½ì–´
TARGET_DATE = datetime.now().strftime("%Y%m%d") #
#TARGET_DATE = "20260210" 

PG_HOST = "localhost"
PG_DB = "datadb"
PG_USER = "datauser"
PG_PASS = "DataPass2024!"

MONGO_URI = "mongodb://datauser:DataPass2024!@127.0.0.1:27017/admin?authSource=admin"

HDFS_BASE = "hdfs://localhost:9000"
RAW_PATH = f"/raw/{BRAND_NAME}/{TARGET_DATE}"
IMAGE_DIR = f"{RAW_PATH}/image"
CONTAINER_NAME = "namenode-main"

spark = SparkSession.builder \
    .appName("FashionBatchJobTopTen") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1") \
    .getOrCreate()

# --- [2. UDF ì •ì˜: Toptenìš© í•„í„°ë§ (í•„ìš”ì‹œ ì‚¬ìš©, í˜„ì¬ëŠ” ëª¨ë‘ ìœ ì§€)] ---
# ëª¨ë“  ì •ë³´ë¥¼ ë„£ê¸°ë¡œ í•˜ì…¨ìœ¼ë¯€ë¡œ, JSON ì§ë ¬í™”ë§Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
def process_topten_material(material_json_str):
    if not material_json_str:
        return "{}"
    try:
        # íŠ¹ë³„í•œ í•„í„°ë§ ì—†ì´ JSON ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ê±°ë‚˜ ì •ë¦¬ë§Œ ìˆ˜í–‰
        return material_json_str 
    except:
        return "{}"

process_udf = udf(process_topten_material, StringType())

# --- [3. PostgreSQLì—ì„œ í˜„ì¬ ì‹œí€€ìŠ¤ ì¡°íšŒ] ---
seq_df = spark.read.format("jdbc") \
    .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
    .option("dbtable", "brand_sequences") \
    .option("user", PG_USER) \
    .option("password", PG_PASS) \
    .option("driver", "org.postgresql.Driver") \
    .load()

# DBì—ëŠ” 'TOPTEN'ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.
row = seq_df.filter(col("brand_name") == BRAND_NAME.upper()).select("last_seq").collect()
start_seq = row[0]['last_seq'] + 1 if row else 1
print(f"ğŸš€ Starting {BRAND_NAME.upper()} job from sequence: {start_seq}")

# --- [4. ETL ë¡œì§] ---
input_path = f"{HDFS_BASE}{RAW_PATH}/*.json"
raw_df = spark.read.option("multiLine", "true") \
                   .option("inferSchema", "true") \
                   .json(input_path) \
                   .withColumn("file_path", input_file_name())

windowSpec = Window.partitionBy(lit(BRAND_NAME)).orderBy(col("goodsNo"))

# Topten íŒŒì¼ëª… ì˜ˆì‹œ: topten_men_outer_MSF4KG1001BR.json
# ê·¸ë£¹ 1: men, ê·¸ë£¹ 2: outer
processed_df = raw_df.withColumn("idx", row_number().over(windowSpec)) \
    .withColumn("product_id", format_string(f"{BRAND_PREFIX}%04d", col("idx").cast("int") + start_seq - 1)) \
    .withColumn("img_hdfs_path", concat(lit(IMAGE_DIR), lit("/"), col("goodsNo"), lit(".jpg"))) \
    .withColumn("gender", lower(regexp_extract(col("file_path"), r"topten_([^_]+)_", 1))) \
    .withColumn("sub_category", lower(regexp_extract(col("file_path"), r"topten_[^_]+_([^_]+)_", 1))) \
    .withColumn("category_code", concat(col("gender"), lit("_"), col("sub_category")))

processed_df.cache()
total_count = processed_df.count()

# --- [5. PostgreSQL ì ì¬] ---
pg_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("goodsNm").alias("prod_name"),
    col("category_code"),
    coalesce(col("price").cast("int"), lit(0)).alias("base_price"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt"),
    current_timestamp().alias("update_dt")
)

pg_data.write.format("jdbc") \
    .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
    .option("dbtable", "products") \
    .option("user", PG_USER) \
    .option("password", PG_PASS) \
    .option("driver", "org.postgresql.Driver") \
    .mode("append") \
    .save()

# --- [6. MongoDB ì ì¬] ---
mongo_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("goodsNm").alias("prod_name"),
    # goodsMaterial ì „ì²´ë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì ì¬
    to_json(col("goodsMaterial")).alias("detail_desc"),
    coalesce(col("price").cast("int"), lit(0)).alias("base_price"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt")
)

mongo_data.write.format("mongodb") \
    .option("spark.mongodb.write.connection.uri", MONGO_URI) \
    .option("database", "datadb") \
    .option("collection", "product_details") \
    .mode("append") \
    .save()

# --- [7. ì´ë¯¸ì§€ ì²˜ë¦¬] ---

# 7-1. HDFS ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ì„ ê²½ìš°ì—ë§Œ ìƒì„±)
# -p ì˜µì…˜ì„ ì£¼ë©´ ë¶€ëª¨ ë””ë ‰í† ë¦¬(/raw/topten/20260210)ê°€ ìˆì–´ë„ ì—ëŸ¬ ì—†ì´ í•˜ìœ„ í´ë”ê¹Œì§€ ìƒì„±í•©ë‹ˆë‹¤.
mkdir_cmd = f"docker exec -i {CONTAINER_NAME} hdfs dfs -mkdir -p {IMAGE_DIR}"
subprocess.run(mkdir_cmd, shell=True)
print(f"ğŸ“‚ HDFS ê²½ë¡œ í™•ì¸ ë° ìƒì„± ì™„ë£Œ: {IMAGE_DIR}")

image_list = processed_df.select(element_at(col("goodsImages"), 1).alias("main_img"), col("goodsNo")).collect()

for r in image_list:
    if r.main_img and r.goodsNo:
        hdfs_target_path = f"{IMAGE_DIR}/{r.goodsNo}.json" # íŒŒì¼ëª…ì— ë§ì¶° ì €ì¥
        # -qO- ì˜µì…˜ìœ¼ë¡œ í‘œì¤€ì¶œë ¥ìœ¼ë¡œ ë³´ë‚¸ ë’¤ dockerë¥¼ í†µí•´ HDFSì— ë°”ë¡œ ì €ì¥
        cmd = f"wget -qO- --header='User-Agent: Mozilla/5.0' '{r.main_img}' | docker exec -i {CONTAINER_NAME} hdfs dfs -put -f - {IMAGE_DIR}/{r.goodsNo}.jpg"
        subprocess.run(cmd, shell=True)

print(f"ğŸ“¸ {total_count}ê°œì˜ ì´ë¯¸ì§€ íŒŒì¼ì´ {IMAGE_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
# --- [8. ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸] ---
try:
    new_last_seq = start_seq + total_count - 1
    conn = psycopg2.connect(host=PG_HOST, database=PG_DB, user=PG_USER, password=PG_PASS)
    cur = conn.cursor()
    cur.execute("UPDATE brand_sequences SET last_seq = %s WHERE brand_name = %s", 
                (new_last_seq, BRAND_NAME.upper()))
    conn.commit()
    print(f"âœ… brand_sequences ì—…ë°ì´íŠ¸ ì™„ë£Œ: {BRAND_NAME.upper()} -> {new_last_seq}")
except Exception as e:
    print(f"âŒ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
finally:
    if 'cur' in locals(): cur.close()
    if 'conn' in locals(): conn.close()

print(f"âœ… {BRAND_NAME.upper()} ì‘ì—… ì™„ë£Œ ({total_count}ê±´)")
spark.stop()
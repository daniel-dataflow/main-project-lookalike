# fashion_batch_job_8S.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import (lit, col, current_timestamp, concat, format_string, 
                                   input_file_name, regexp_extract, lower, element_at, 
                                   coalesce, to_json, row_number, udf)
from pyspark.sql.window import Window
from pyspark.sql.types import StringType
import json
import subprocess
import psycopg2
from datetime import datetime

# --- [1. ì„¤ì • ì •ë³´] ---
MONGO_IP = "localhost"
BRAND_NAME = "8seconds"
BRAND_PREFIX = "8S"
# í˜„ì¬ ì‹¤í–‰ ì‹œì ì˜ ë‚ ì§œ (HDFS ê²½ë¡œì™€ ì¼ì¹˜í•´ì•¼ í•¨)
TARGET_DATE = datetime.now().strftime("%Y%m%d") #
#TARGET_DATE = "20260210" 

# DB ì ‘ì† ì •ë³´ (ë³¸ì¸ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í™•ì¸)
PG_HOST = "localhost"
PG_DB = "datadb"
PG_USER = "datauser"
PG_PASS = "DataPass2024!"

MONGO_URI = "mongodb://localhost:27017"

HDFS_BASE = "hdfs://localhost:9000"
RAW_PATH = f"/raw/{BRAND_NAME}/{TARGET_DATE}"
IMAGE_DIR = f"{RAW_PATH}/image"
CONTAINER_NAME = "namenode-main"

spark = SparkSession.builder \
    .appName("FashionBatchJob8S") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1") \
    .getOrCreate()

# --- [2. UDF ì •ì˜: ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ í•„í„°ë§] ---
def filter_material_info(material_json_str):
    if not material_json_str:
        return "{}"
    exclude_keywords = ["ìœ ì˜ì‚¬í•­", "ë¶„ìŸí•´ê²°", "ì±…ì„ì", "í™˜ë¶ˆ", "ë°˜í’ˆ", "êµí™˜", "ë°°ì†¡", "ì ‘ìˆ˜ë°©ë²•", "ë¼ë²¨", "SSF SHOP", "ë§¤ì¥í”½ì—…"]
    try:
        data = json.loads(material_json_str)
        filtered_data = {k: v for k, v in data.items() if not any(word in k for word in exclude_keywords)}
        return json.dumps(filtered_data, ensure_ascii=False)
    except:
        return "{}"

filter_udf = udf(filter_material_info, StringType())

# --- [3. PostgreSQLì—ì„œ í˜„ì¬ ì‹œí€€ìŠ¤ ì¡°íšŒ] ---
seq_df = spark.read.format("jdbc") \
    .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
    .option("dbtable", "brand_sequences") \
    .option("user", PG_USER) \
    .option("password", PG_PASS) \
    .option("driver", "org.postgresql.Driver") \
    .load()

row = seq_df.filter(col("brand_name") == BRAND_NAME.upper()).select("last_seq").collect()
start_seq = row[0]['last_seq'] + 1 if row else 1
print(f"ğŸš€ Starting {BRAND_NAME} job from sequence: {start_seq}")

# --- [4. ETL ë¡œì§] ---
input_path = f"{HDFS_BASE}{RAW_PATH}/*.json"
raw_df = spark.read.option("multiLine", "true") \
                   .option("inferSchema", "true") \
                   .option("samplingRatio", "1.0") \
                   .json(input_path) \
                   .withColumn("file_path", input_file_name())

windowSpec = Window.partitionBy(lit(BRAND_NAME)).orderBy(col("goodsNo"))

processed_df = raw_df.withColumn("idx", row_number().over(windowSpec)) \
    .withColumn("product_id", format_string(f"{BRAND_PREFIX}%04d", col("idx").cast("int") + start_seq - 1)) \
    .withColumn("img_hdfs_path", concat(lit(IMAGE_DIR), lit("/"), col("goodsNo"), lit(".jpg"))) \
    .withColumn("gender", lower(regexp_extract(col("file_path"), r"8seconds_([^_]+)_", 1))) \
    .withColumn("sub_category", lower(regexp_extract(col("file_path"), r"8seconds_[^_]+_([^_]+)_", 1))) \
    .withColumn("category_code", concat(col("gender"), lit("_"), col("sub_category")))

processed_df.cache()
total_count = processed_df.count()

# --- [5. PostgreSQL ì ì¬] ---
pg_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),  # <-- ì´ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”!
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("goodsNm").alias("prod_name"),
    col("category_code"),
    coalesce(col("price").cast("int"), lit(0)).alias("base_price"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt"),
    current_timestamp().alias("update_dt")
)

pg_data.write.format("jdbc") \
    .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
    .option("dbtable", "products") \
    .option("user", PG_USER) \
    .option("password", PG_PASS) \
    .option("driver", "org.postgresql.Driver") \
    .mode("append") \
    .save()

# --- [6. MongoDB ì ì¬] ---
mongo_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("goodsNm").alias("prod_name"),
    filter_udf(to_json(col("goodsMaterial"))).alias("detail_desc"),
    coalesce(col("price").cast("int"), lit(0)).alias("base_price"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt")
)

mongo_data.repartition(1).write.format("mongodb") \
    .option("spark.mongodb.write.connection.uri", "mongodb://datauser:DataPass2024!@127.0.0.1:27017/admin?authSource=admin") \
    .option("database", "datadb") \
    .option("collection", "product_details") \
    .option("maxBatchSize", 20) \
    .option("localThreshold", "0") \
    .option("serverSelectionTimeoutMS", "5000") \
    .mode("append") \
    .save()

# --- [7. ì´ë¯¸ì§€ ì²˜ë¦¬] ---

# í¬ë¡¤ëŸ¬ê°€ ë‚ ì§œ í´ë”ê¹Œì§€ë§Œ ë§Œë“¤ì—ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ /image í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
mkdir_cmd = f"docker exec -i {CONTAINER_NAME} hdfs dfs -mkdir -p {IMAGE_DIR}"
subprocess.run(mkdir_cmd, shell=True)
print(f"ğŸ“‚ [8Seconds] HDFS ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„± ì™„ë£Œ: {IMAGE_DIR}")

# goodsImages ë°°ì—´ì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ
image_list = processed_df.select(element_at(col("goodsImages"), 1).alias("main_img"), col("goodsNo")).collect()

for r in image_list:
    if r.main_img and r.goodsNo:
        hdfs_target_path = f"{IMAGE_DIR}/{r.goodsNo}.jpg"
        cmd = f"wget -qO- --header='User-Agent: Mozilla/5.0' '{r.main_img}' | docker exec -i {CONTAINER_NAME} hdfs dfs -put - {hdfs_target_path}"
        subprocess.run(cmd, shell=True)

# --- [8. ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸] ---
# --- [5. PostgreSQL ì ì¬ ì™„ë£Œ í›„] ---
# ... (ê¸°ì¡´ pg_data.write.save() ì½”ë“œ ë°”ë¡œ ì•„ë˜ì— ì¶”ê°€)

try:
    
    # 1. ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ê³„ì‚°
    new_last_seq = start_seq + total_count - 1
    
    # 2. DB ì—°ê²°
    conn = psycopg2.connect(
        host=PG_HOST, 
        database=PG_DB, 
        user=PG_USER, 
        password=PG_PASS
    )
    cur = conn.cursor()
    
    # 3. ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤í–‰
    cur.execute(
        "UPDATE brand_sequences SET last_seq = %s WHERE brand_name = %s", 
        (new_last_seq, BRAND_NAME.upper())
    )
    
    # 4. ë³€ê²½ì‚¬í•­ í™•ì • ë° ì¢…ë£Œ
    conn.commit()
    print(f"âœ… brand_sequences ì—…ë°ì´íŠ¸ ì™„ë£Œ: {BRAND_NAME.upper()} -> {new_last_seq}")
    
except Exception as e:
    print(f"âŒ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ ì‘ì—… ì „ì²´ì˜ ì •í•©ì„±ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¡œê·¸ë¥¼ ë‚¨ê¸°ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
finally:
    if 'cur' in locals(): cur.close()
    if 'conn' in locals(): conn.close()


print(f"âœ… {BRAND_NAME.upper()} {total_count}ê±´ PostgreSQL/MongoDB ì ì¬ ë° ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ!")
spark.stop()





# fashion_batch_job_UQ.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import (lit, col, current_timestamp, concat, format_string, 
                                   input_file_name, regexp_extract, lower, 
                                   coalesce, to_json, row_number, explode)
from pyspark.sql.window import Window
import subprocess
import psycopg2

# --- [1. ì„¤ì • ì •ë³´] ---
BRAND_NAME = "uniqlo"
BRAND_PREFIX = "UQ"
TARGET_DATE = datetime.now().strftime("%Y%m%d") #
#TARGET_DATE = "20260210" 

PG_HOST = "localhost"
PG_DB = "datadb"
PG_USER = "datauser"
PG_PASS = "DataPass2024!"

MONGO_URI = "mongodb://datauser:DataPass2024!@127.0.0.1:27017/admin?authSource=admin"

HDFS_BASE = "hdfs://localhost:9000"
RAW_PATH = f"/raw/{BRAND_NAME}/{TARGET_DATE}"
IMAGE_DIR = f"{RAW_PATH}/image"
CONTAINER_NAME = "namenode-main"

spark = SparkSession.builder \
    .appName("FashionBatchJobUniqlo") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1") \
    .getOrCreate()

# --- [2. PostgreSQLì—ì„œ í˜„ì¬ ì‹œí€€ìŠ¤ ì¡°íšŒ] ---
seq_df = spark.read.format("jdbc") \
    .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
    .option("dbtable", "brand_sequences") \
    .option("user", PG_USER) \
    .option("password", PG_PASS) \
    .option("driver", "org.postgresql.Driver") \
    .load()

row = seq_df.filter(col("brand_name") == BRAND_NAME.upper()).select("last_seq").collect()
start_seq = row[0]['last_seq'] + 1 if row else 1
print(f"ğŸš€ Starting {BRAND_NAME.upper()} job from sequence: {start_seq}")

# --- [3. ETL ë¡œì§] ---
input_path = f"{HDFS_BASE}{RAW_PATH}/*.json"
raw_df = spark.read.option("multiLine", "true").json(input_path) \
    .withColumn("file_path", input_file_name())

windowSpec = Window.partitionBy(lit(BRAND_NAME)).orderBy(col("goodsNo"))

processed_df = raw_df.withColumn("idx", row_number().over(windowSpec)) \
    .withColumn("product_id", format_string(f"{BRAND_PREFIX}%04d", col("idx").cast("int") + start_seq - 1)) \
    .withColumn("img_hdfs_path", concat(lit(IMAGE_DIR), lit("/"), col("goodsNo"), lit(".jpg"))) \
    .withColumn("gender", lower(regexp_extract(col("file_path"), r"uniqlo_([^_]+)_", 1))) \
    .withColumn("sub_category", lower(regexp_extract(col("file_path"), r"uniqlo_[^_]+_([^_]+)_", 1))) \
    .withColumn("category_code", concat(col("gender"), lit("_"), col("sub_category")))

processed_df.cache()
total_count = processed_df.count()

# --- [4. PostgreSQL ì ì¬] ---
pg_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("goodsNm").alias("prod_name"),
    col("category_code"),
    coalesce(col("price").cast("int"), lit(0)).alias("base_price"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt"),
    current_timestamp().alias("update_dt")
)

pg_data.write.format("jdbc") \
    .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
    .option("dbtable", "products") \
    .option("user", PG_USER) \
    .option("password", PG_PASS) \
    .option("driver", "org.postgresql.Driver") \
    .mode("append") \
    .save()
print("âœ… PostgreSQL ì ì¬ ì™„ë£Œ")

# --- [5. MongoDB ì ì¬] ---
mongo_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("goodsNm").alias("prod_name"),
    to_json(col("goodsMaterial")).alias("detail_desc"), # Uniqloì˜ ë³µì¡í•œ ì†Œì¬ ê°ì²´ë¥¼ JSON ë¬¸ìì—´ë¡œ ì €ì¥
    coalesce(col("price").cast("int"), lit(0)).alias("base_price"),
    col("img_hdfs_path"),
    current_timestamp().alias("create_dt")
)

mongo_data.write.format("mongodb") \
    .option("spark.mongodb.write.connection.uri", MONGO_URI) \
    .option("database", "datadb") \
    .option("collection", "product_details") \
    .mode("append") \
    .save()
print("âœ… MongoDB ì ì¬ ì™„ë£Œ")

# --- [6. ì´ë¯¸ì§€ ì²˜ë¦¬ (colors ë‚´ ëª¨ë“  icon_url ìˆ˜ì§‘)] ---
subprocess.run(f"docker exec -i {CONTAINER_NAME} hdfs dfs -mkdir -p {IMAGE_DIR}", shell=True)

# colors ë°°ì—´ì„ explodeí•˜ì—¬ ëª¨ë“  color_codeì™€ icon_url ìŒì„ ì¶”ì¶œ í›„ ì¤‘ë³µ ì œê±°
color_images = processed_df.select(
    explode(col("colors")).alias("color_info")
).select(
    col("color_info.color_code"),
    col("color_info.icon_url")
).distinct().collect()

print(f"ğŸ“¸ ì´ {len(color_images)}ê°œì˜ ê³ ìœ  ì»¬ëŸ¬ ì•„ì´ì½˜ ì´ë¯¸ì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.")

for r in color_images:
    if r.icon_url:
        # URLì—ì„œ ì›ë³¸ íŒŒì¼ëª… ì¶”ì¶œ (ì˜ˆ: goods_09_481666_chip.jpg)
        file_name = r.icon_url.split('/')[-1] 
        cmd = f"wget -qO- --header='User-Agent: Mozilla/5.0' '{r.icon_url}' | docker exec -i {CONTAINER_NAME} hdfs dfs -put -f - {IMAGE_DIR}/{file_name}"
        subprocess.run(cmd, shell=True)

# --- [7. ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸] ---
try:
    new_last_seq = start_seq + total_count - 1
    conn = psycopg2.connect(host=PG_HOST, database=PG_DB, user=PG_USER, password=PG_PASS)
    cur = conn.cursor()
    cur.execute("UPDATE brand_sequences SET last_seq = %s WHERE brand_name = %s", 
                (new_last_seq, BRAND_NAME.upper()))
    conn.commit()
    print(f"âœ… brand_sequences ì—…ë°ì´íŠ¸ ì™„ë£Œ: {BRAND_NAME.upper()} -> {new_last_seq}")
except Exception as e:
    print(f"âŒ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
finally:
    if 'cur' in locals(): cur.close()
    if 'conn' in locals(): conn.close()

print(f"ğŸ {BRAND_NAME.upper()} ì‘ì—… ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œ ({total_count}ê±´)")
spark.stop()
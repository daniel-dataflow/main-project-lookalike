from pyspark.sql import SparkSession
from pyspark.sql.functions import (lit, col, current_timestamp, concat, format_string, 
                                   input_file_name, regexp_extract, lower, concat_ws, element_at, 
                                   coalesce, to_json, row_number)
from pyspark.sql.window import Window
import psycopg2
import datetime
import os
import sys
import requests
from pytz import timezone

# ìŠ¤íŒŒí¬ ì›Œì»¤ ë‚´ íŒŒì´ì¬ ë²„ì „ ì—ëŸ¬ ë°©ì§€
os.environ["PYSPARK_PYTHON_VERSION_CHECK"] = "0"

# --- [1. ì„¤ì • ì •ë³´] ---
MONGO_IP = "mongo-main"
BRAND_NAME = "8seconds"
BRAND_PREFIX = "8S"

kst = timezone('Asia/Seoul')
TARGET_DATE = datetime.datetime.now(kst).strftime("%Y%m%d")

PG_HOST = "postgres-main"  
PG_DB = "datadb"       
PG_USER = "datauser"
PG_PASS = "DataPass2026!"  

MONGO_USER = "datauser"
MONGO_PASS = "DataPass2026!"

HDFS_BASE = "hdfs://namenode-main:9000"
RAW_PATH = f"/raw/{BRAND_NAME}/{TARGET_DATE}"
IMAGE_DIR = f"/raw/{BRAND_NAME}/image"

# WebHDFS ì„¤ì •
WEBHDFS_HOST = "namenode-main"
WEBHDFS_PORT = "9870"
WEBHDFS_USER = "root"

spark = SparkSession.builder \
    .appName("FashionBatchJob8S") \
    .config("spark.mongodb.write.connection.uri", f"mongodb://{MONGO_USER}:{MONGO_PASS}@{MONGO_IP}:27017/{PG_DB}?authSource=admin") \
    .getOrCreate()

# --- [2. PostgreSQL ì‹œí€€ìŠ¤ ê´€ë¦¬] ---
try:
    # brand_sequences í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±
    init_conn = psycopg2.connect(host=PG_HOST, database=PG_DB, user=PG_USER, password=PG_PASS)
    init_cur = init_conn.cursor()
    init_cur.execute("""
        CREATE TABLE IF NOT EXISTS brand_sequences (
            brand_name VARCHAR(50) PRIMARY KEY,
            last_seq INTEGER DEFAULT 0
        );
    """)
    init_conn.commit()
    
    # í˜„ì¬ ì‹œí€€ìŠ¤ ì¡°íšŒ
    seq_df = spark.read.format("jdbc") \
        .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
        .option("dbtable", "brand_sequences") \
        .option("user", PG_USER) \
        .option("password", PG_PASS) \
        .option("driver", "org.postgresql.Driver") \
        .load()

    row = seq_df.filter(col("brand_name") == BRAND_NAME.upper()).select("last_seq").collect()

    if not row:
        print(f"âœ¨ {BRAND_NAME} sequence not found. Registering...")
        init_cur.execute("INSERT INTO brand_sequences (brand_name, last_seq) VALUES (%s, 0)", (BRAND_NAME.upper(),))
        init_conn.commit()
        start_seq = 1
    else:
        start_seq = row[0]['last_seq'] + 1
    init_cur.close()
    init_conn.close()

except Exception as e:
    print(f"âš ï¸ Sequence check failed: {e}. Starting from 1.")
    start_seq = 1

print(f"ğŸš€ Job Start | Brand: {BRAND_NAME} | Seq: {start_seq} | Date: {TARGET_DATE}")

# --- [3. ETL ë¡œì§] ---
input_path = f"{HDFS_BASE}{RAW_PATH}/*.json"
raw_df = spark.read.option("multiLine", "true") \
                   .option("inferSchema", "true") \
                   .json(input_path) \
                   .withColumn("file_path", input_file_name())

windowSpec = Window.partitionBy(lit(BRAND_NAME)).orderBy(col("goodsNo"))

# ë°ì´í„° ì²˜ë¦¬ (íŒŒì¼ëª… ìƒì„± ë° HDFS ê²½ë¡œ ì§€ì •)
processed_df = raw_df.withColumn("idx", row_number().over(windowSpec)) \
    .withColumn("product_id", format_string(f"{BRAND_PREFIX}%04d", col("idx").cast("int") + start_seq - 1)) \
    .withColumn("target_img_url", 
        coalesce(
            element_at(col("goodsImages"), 26), # 1ìˆœìœ„: ê³ í™”ì§ˆ
            element_at(col("goodsImages"), 2)   # 2ìˆœìœ„: ì¸ë„¤ì¼
        )
    ) \
    .withColumn("gender", lower(regexp_extract(col("file_path"), r"8seconds_([^_]+)_", 1))) \
    .withColumn("sub_category", lower(regexp_extract(col("file_path"), r"8seconds_[^_]+_([^_]+)_", 1))) \
    .withColumn("category_code", concat(col("gender"), lit("_"), col("sub_category"))) \
    .withColumn("new_filename", concat_ws("_", lit(BRAND_NAME.lower()), col("gender"), col("sub_category"), col("goodsNo"))) \
    .withColumn("img_hdfs_path", concat(lit(IMAGE_DIR), lit("/"), col("new_filename"), lit(".jpg")))

processed_df.cache()
total_count = processed_df.count()

if total_count == 0:
    print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    spark.stop()
    sys.exit(0)

print(f"ğŸ“Š ì²˜ë¦¬í•  ë°ì´í„°: {total_count} ê±´")

# --- [4. PostgreSQL ì ì¬] ---
pg_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand"),
    col("goodsNm").alias("product_name"),
    col("category_code").alias("category"),
    coalesce(col("price").cast("int"), lit(0)).alias("price"),
    col("img_hdfs_path").alias("local_image_path"),
    current_timestamp().alias("created_at")
)

pg_data.write.format("jdbc") \
    .option("url", f"jdbc:postgresql://{PG_HOST}:5432/{PG_DB}") \
    .option("dbtable", "fashion_products") \
    .option("user", PG_USER) \
    .option("password", PG_PASS) \
    .option("driver", "org.postgresql.Driver") \
    .mode("append") \
    .save()

# --- [5. MongoDB ì ì¬] ---
mongo_data = processed_df.select(
    col("product_id"),
    col("goodsNo").alias("model_code"),
    lit(BRAND_NAME.upper()).alias("brand_name"),
    col("goodsNm").alias("prod_name"),
    to_json(col("goodsMaterial")).alias("detail_desc"),
    col("img_hdfs_path"),
    col("goodsImages").alias("all_images"),
    current_timestamp().alias("create_dt")
)

mongo_data.write.format("mongodb") \
    .option("spark.mongodb.write.database", PG_DB) \
    .option("spark.mongodb.write.collection", "fashion_metadata") \
    .mode("append") \
    .save()

# --- [6. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° WebHDFS ì €ì¥] ---

def upload_to_hdfs_via_webhdfs(local_data, hdfs_path, filename):
    """WebHDFS REST APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        # 1. CREATE ìš”ì²­ (ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ë°›ê¸°)
        create_url = f"http://{WEBHDFS_HOST}:{WEBHDFS_PORT}/webhdfs/v1{hdfs_path}/{filename}?op=CREATE&overwrite=true&user.name={WEBHDFS_USER}"
        
        response = requests.put(create_url, allow_redirects=False)
        
        if response.status_code == 307:  # Temporary Redirect
            # 2. DataNodeë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ URLì— PUT ìš”ì²­
            datanode_url = response.headers['Location']
            put_response = requests.put(datanode_url, data=local_data)
            
            if put_response.status_code in [200, 201]:
                return True
            else:
                print(f"âŒ HDFS ì—…ë¡œë“œ ì‹¤íŒ¨ (DataNode): {put_response.status_code}")
                return False
        else:
            print(f"âŒ HDFS CREATE ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ HDFS ì—…ë¡œë“œ ì˜ˆì™¸: {e}")
        return False

# ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ìƒì„± (WebHDFS mkdir)
try:
    mkdir_url = f"http://{WEBHDFS_HOST}:{WEBHDFS_PORT}/webhdfs/v1{IMAGE_DIR}?op=MKDIRS&user.name={WEBHDFS_USER}"
    requests.put(mkdir_url)
except:
    pass

image_list = processed_df.select("target_img_url", "new_filename", "goodsNo").collect()
print(f"ğŸ“¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {len(image_list)} ê±´")

success_count = 0
fail_count = 0

for r in image_list:
    if r.target_img_url and r.new_filename:
        filename = f"{r.new_filename}.jpg"
        try:
            # 1. ì‡¼í•‘ëª° ì„œë²„ì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ê°€ì§œ ë¸Œë¼ìš°ì € í—¤ë” í•„ìˆ˜)
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(r.target_img_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                # 2. ë‹¤ìš´ë°›ì€ ì´ë¯¸ì§€ë¥¼ WebHDFSë¥¼ í†µí•´ í•˜ë‘¡ìœ¼ë¡œ ì „ì†¡
                if upload_to_hdfs_via_webhdfs(response.content, IMAGE_DIR, filename):
                    success_count += 1
                else:
                    fail_count += 1
                    print(f"âŒ HDFS ì—…ë¡œë“œ ì‹¤íŒ¨: {r.goodsNo}")
            else:
                fail_count += 1
                print(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({r.goodsNo}): HTTP {response.status_code}")
                
        except Exception as e:
            fail_count += 1
            print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜ˆì™¸ ë°œìƒ ({r.goodsNo}): {e}")

print(f"ğŸ“¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {success_count} / ì‹¤íŒ¨ {fail_count}")

# --- [7. ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸] ---
try:
    new_last_seq = start_seq + total_count - 1
    conn = psycopg2.connect(host=PG_HOST, database=PG_DB, user=PG_USER, password=PG_PASS)
    cur = conn.cursor()
    cur.execute("UPDATE brand_sequences SET last_seq = %s WHERE brand_name = %s", (new_last_seq, BRAND_NAME.upper()))
    conn.commit()
    cur.close()
    conn.close()
    print(f"âœ… ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {new_last_seq}")
except Exception as e:
    print(f"âŒ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")

print(f"ğŸ {BRAND_NAME.upper()} ì¡ ì™„ë£Œ! ì´ {total_count} ê±´ ì²˜ë¦¬ë¨.")
spark.stop()
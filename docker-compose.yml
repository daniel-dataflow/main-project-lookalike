# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤íŠ¸ì›Œí¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
networks:
  main-network:
    driver: bridge

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³¼ë¥¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
volumes:
  postgres_data:
    name: main_postgres_data
  mongo_data:
    name: main_mongo_data
  redis_data:
    name: main_redis_data
  es_data:
    name: main_es_data
  zookeeper_data:
    name: main_zookeeper_data
  zookeeper_logs:
    name: main_zookeeper_logs
  kafka_data:
    name: main_kafka_data
  spark_master_logs:
    name: main_spark_master_logs
  spark_worker1_logs:
    name: main_spark_worker1_logs
  namenode_data:
    name: main_namenode_data
  datanode_data:
    name: main_datanode_data
  airflow_logs:
    name: main_airflow_logs
  airflow_plugins:
    name: main_airflow_plugins

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
services:
  # ============================================================
  # 1. PostgreSQL
  # ============================================================
  postgresql:
    image: postgres:15-alpine
    container_name: postgres-main
    hostname: postgresql
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      TZ: ${TZ}
    ports:
      - "${EXTERNAL_POSTGRES_PORT}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ============================================================
  # 1-1. DB ì´ˆê¸°í™” (Airflow DB + í”„ë¡œì íŠ¸ í…Œì´ë¸”)
  # ============================================================
  init-db:
    image: postgres:15-alpine
    container_name: init-db-main
    depends_on:
      postgresql: { condition: service_healthy }
    env_file: [ .env ]
    environment:
      PGPASSWORD: ${POSTGRES_PASSWORD}
    command: >
      bash -c "
        echo 'ðŸ”§ DB ì´ˆê¸°í™” ì‹œìž‘...'

        psql -h postgresql -U $${POSTGRES_USER} -d postgres -tc \"SELECT 1 FROM pg_database WHERE datname='$${AIRFLOW_DB}'\" | grep -q 1 || createdb -h postgresql -U $${POSTGRES_USER} $${AIRFLOW_DB}
        echo 'âœ… Airflow DB ì¤€ë¹„ ì™„ë£Œ'

        psql -h postgresql -U $${POSTGRES_USER} -d $${POSTGRES_DB} <<'EOF'
          CREATE TABLE IF NOT EXISTS users (
            user_id VARCHAR(50) PRIMARY KEY, password VARCHAR(255), name VARCHAR(50),
            email VARCHAR(100) UNIQUE, role VARCHAR(20) DEFAULT 'USER',
            provider VARCHAR(20) DEFAULT 'email', social_id VARCHAR(255),
            profile_image VARCHAR(512),
            last_login TIMESTAMP DEFAULT NOW(), create_dt TIMESTAMP DEFAULT NOW(), update_dt TIMESTAMP DEFAULT NOW()
          );
          CREATE UNIQUE INDEX IF NOT EXISTS idx_users_social ON users(provider, social_id);
          CREATE TABLE IF NOT EXISTS posts (
            post_id BIGSERIAL PRIMARY KEY, title VARCHAR(200) NOT NULL, content TEXT,
            author_id VARCHAR(50) REFERENCES users(user_id), view_count INTEGER DEFAULT 0,
            is_notice BOOLEAN DEFAULT FALSE, create_dt TIMESTAMP DEFAULT NOW(), update_dt TIMESTAMP DEFAULT NOW()
          );
          CREATE TABLE IF NOT EXISTS comments (
            comment_id BIGSERIAL PRIMARY KEY, post_id BIGINT REFERENCES posts(post_id) ON DELETE CASCADE,
            author_id VARCHAR(50) REFERENCES users(user_id), comment_text TEXT, create_dt TIMESTAMP DEFAULT NOW()
          );
          CREATE INDEX IF NOT EXISTS idx_comments_post_id ON comments(post_id);
          CREATE TABLE IF NOT EXISTS products (
            product_id BIGSERIAL PRIMARY KEY, origine_prod_id VARCHAR(50), model_code VARCHAR(50),
            prod_name VARCHAR(50), base_price INTEGER, category_code VARCHAR(50),
            img_hdfs_path VARCHAR(512), create_dt TIMESTAMP DEFAULT NOW(), update_dt TIMESTAMP DEFAULT NOW()
          );
          CREATE TABLE IF NOT EXISTS naver_prices (
            nprice_id BIGSERIAL PRIMARY KEY, product_id BIGINT REFERENCES products(product_id),
            rank SMALLINT, price INTEGER, mall_name VARCHAR(100), mall_url VARCHAR(500),
            create_dt TIMESTAMP DEFAULT NOW()
          );
          CREATE INDEX IF NOT EXISTS idx_naver_prices_product_id ON naver_prices(product_id);
          CREATE TABLE IF NOT EXISTS product_features (
            product_id BIGINT PRIMARY KEY REFERENCES products(product_id),
            detected_desc VARCHAR(1000), create_dt TIMESTAMP DEFAULT NOW()
          );
          CREATE TABLE IF NOT EXISTS search_logs (
            log_id BIGSERIAL PRIMARY KEY, user_id VARCHAR(50) REFERENCES users(user_id),
            input_img_path VARCHAR(512), input_text TEXT, applied_category VARCHAR(50),
            nprice_id BIGINT REFERENCES naver_prices(nprice_id),
            create_dt TIMESTAMP DEFAULT NOW(), update_dt TIMESTAMP DEFAULT NOW()
          );
          CREATE INDEX IF NOT EXISTS idx_search_logs_user_id ON search_logs(user_id);
          CREATE INDEX IF NOT EXISTS idx_search_logs_nprice_id ON search_logs(nprice_id);
      EOF
        echo 'âœ… í”„ë¡œì íŠ¸ í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ'
        echo 'ðŸš€ DB ì´ˆê¸°í™” ì™„ë£Œ!'
      "
    networks: [ main-network ]
    restart: "no"

  # ============================================================
  # 2. MongoDB
  # ============================================================
  mongodb:
    image: mongo:7.0
    container_name: mongo-main
    hostname: mongodb
    env_file:
      - .env
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGODB_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGODB_PASSWORD}
      TZ: ${TZ}
    ports:
      - "${EXTERNAL_MONGODB_PORT}:27017"
    volumes:
      - mongo_data:/data/db
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: [ "CMD-SHELL", "mongosh --eval \"db.adminCommand('ping')\" --quiet" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 15s

  # ============================================================
  # 3. Redis
  # ============================================================
  redis:
    image: redis:7-alpine
    container_name: redis-main
    hostname: redis
    env_file:
      - .env
    command: redis-server --requirepass ${REDIS_PASSWORD}
    ports:
      - "${EXTERNAL_REDIS_PORT}:6379"
    volumes:
      - redis_data:/data
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli -a $${REDIS_PASSWORD} ping | grep PONG" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s

  # ============================================================
  # 4. Elasticsearch
  # ============================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch-main
    hostname: elasticsearch
    env_file:
      - .env
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - TZ=${TZ}
    ports:
      - "${EXTERNAL_ELASTICSEARCH_PORT}:9200"
      - "9300:9300"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"\\|\"status\":\"yellow\"'" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ============================================================
  # 5. Zookeeper
  # ============================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper-main
    hostname: zookeeper
    env_file:
      - .env
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
      TZ: ${TZ}
    ports:
      - "${ZOOKEEPER_PORT}:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # ============================================================
  # 6. Kafka
  # ============================================================
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-main
    hostname: kafka
    depends_on:
      - zookeeper
    env_file:
      - .env
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:${KAFKA_PORT}
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      TZ: ${TZ}
    ports:
      - "${KAFKA_PORT}:9092"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # ============================================================
  # 7. Hadoop NameNode
  # ============================================================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode-main
    hostname: namenode
    env_file:
      - .env
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - TZ=${TZ}
    ports:
      - "${HADOOP_NAMENODE_WEBUI_PORT}:9870"
      - "${HDFS_NAMENODE_PORT}:9000"
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G

  # ============================================================
  # 8. Hadoop DataNode
  # ============================================================
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode-main
    hostname: datanode
    depends_on:
      - namenode
    env_file:
      - .env
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - SERVICE_PRECONDITION=namenode:9870
      - TZ=${TZ}
    ports:
      - "${HADOOP_DATANODE_WEBUI_PORT}:9864"
    volumes:
      - datanode_data:/hadoop/dfs/data
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G

  # ============================================================
  # 9. Hadoop ResourceManager
  # ============================================================
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager-main
    hostname: resourcemanager
    depends_on:
      - namenode
    env_file:
      - .env
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_resourcemanager_bind___host=0.0.0.0
      - SERVICE_PRECONDITION=namenode:9870
      - TZ=${TZ}
    ports:
      - "${HADOOP_RESOURCEMANAGER_WEBUI_PORT}:8088"
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G

  # ============================================================
  # 10. Hadoop NodeManager
  # ============================================================
  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager-main
    hostname: nodemanager
    depends_on:
      - resourcemanager
    env_file:
      - .env
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_nodemanager_bind___host=0.0.0.0
      - SERVICE_PRECONDITION=namenode:9870 resourcemanager:8088
      - TZ=${TZ}
    ports:
      - "${HADOOP_NODEMANAGER_WEBUI_PORT}:8042"
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G

  # ============================================================
  # 11. Spark Master
  # ============================================================
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master-main
    hostname: spark-master
    depends_on:
      - namenode
    env_file:
      - .env
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_NO_DAEMONIZE=true
      - TZ=${TZ}
    ports:
      - "${SPARK_MASTER_WEBUI_PORT}:8080"
      - "${SPARK_MASTER_PORT}:7077"
      - "${SPARK_REST_PORT}:6066"
    volumes:
      - ./data-pipeline/spark/jobs:/opt/spark-jobs
      - ./data-pipeline/spark/notebooks:/opt/spark-notebooks
      - spark_master_logs:/spark/logs
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M

  # ============================================================
  # 12. Spark Worker
  # ============================================================
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1-main
    hostname: spark-worker-1
    depends_on:
      - spark-master
    env_file:
      - .env
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8081
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_NO_DAEMONIZE=true
      - TZ=${TZ}
    ports:
      - "${SPARK_WORKER1_WEBUI_PORT}:8081"
    volumes:
      - ./data-pipeline/spark/jobs:/opt/spark-jobs
      - ./data-pipeline/spark/notebooks:/opt/spark-notebooks
      - spark_worker1_logs:/spark/logs
    networks:
      - main-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  # ============================================================
  # 13. FastAPI (Dockerfile ì—†ì´)
  # ============================================================
  fastapi:
    image: python:3.11-slim
    container_name: fastapi-main
    hostname: fastapi
    depends_on:
      postgresql: { condition: service_healthy }
      mongodb: { condition: service_healthy }
      redis: { condition: service_healthy }
    env_file: [ .env ]
    environment:
      - APP_ENV=${APP_ENV}
      - POSTGRES_HOST=postgresql
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - MONGODB_HOST=mongodb
      - REDIS_HOST=redis
      - TZ=${TZ}
      - PYTHONUNBUFFERED=1
    working_dir: /app
    ports: [ "${FASTAPI_PORT}:8900" ]
    volumes:
      - .:/app
      - ./miniconda3:/opt/miniconda3
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    command: >
      bash -c "/opt/miniconda3/envs/ml-env/bin/python -m uvicorn web.backend.app.main:app --host 0.0.0.0 --port 8900"     
    healthcheck:
      test: [ "CMD-SHELL", "python -c 'import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.connect((\"localhost\", 8900))' || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s

  # ============================================================
  # 14. Airflow Webserver (Dockerfile ì—†ì´)
  # ============================================================
  airflow-webserver:
    image: apache/airflow:3.0.3-python3.11
    container_name: airflow-webserver-main
    hostname: airflow-webserver
    user: "50000:0"
    depends_on:
      init-db: { condition: service_completed_successfully }
    env_file: [ .env ]
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__AUTH__MANAGER=airflow.auth.managers.simple.simple_auth_manager.SimpleAuthManager
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.default
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/${AIRFLOW_DB}
      - AIRFLOW__CORE__FERNET_KEY=ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - TZ=${TZ}
    ports: [ "${AIRFLOW_WEBSERVER_PORT}:8080" ]
    volumes:
      - ./data-pipeline/airflow:/opt/airflow
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    command: >
      bash -c "
        pip install --no-cache-dir apache-airflow-providers-postgres psycopg2-binary
        airflow db migrate &&
        airflow api-server"
    healthcheck:
      test: [ "CMD-SHELL", "python -c \"import socket, sys; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); result = s.connect_ex(('localhost', 8080)); sys.exit(result)\"" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # ============================================================
  # 15. Airflow Scheduler (Dockerfile ì—†ì´)
  # ============================================================
  airflow-scheduler:
    image: apache/airflow:3.0.3-python3.11
    container_name: airflow-scheduler-main
    hostname: airflow-scheduler
    user: "50000:0"
    depends_on:
      airflow-webserver: { condition: service_started }
    env_file: [ .env ]
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__AUTH__MANAGER=airflow.auth.managers.simple.simple_auth_manager.SimpleAuthManager
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/${AIRFLOW_DB}
      - AIRFLOW__CORE__FERNET_KEY=ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - TZ=${TZ}
    volumes:
      - ./data-pipeline/airflow:/opt/airflow
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks: [ main-network ]
    restart: unless-stopped
    command: >
      bash -c "pip install --no-cache-dir apache-airflow-providers-postgres apache-airflow-providers-apache-spark psycopg2-binary &&
               airflow scheduler"
    healthcheck:
      test: [ "CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\"" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

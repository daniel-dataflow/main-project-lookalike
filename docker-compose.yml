networks:
  main-network:
    driver: bridge
    # name: main-project-network

volumes:
  postgres_data: { name: main_postgres_data }
  mongo_data: { name: main_mongo_data }
  redis_data: { name: main_redis_data }
  es_data: { name: main_es_data }
  zookeeper_data: { name: main_zookeeper_data }
  zookeeper_logs: { name: main_zookeeper_logs }
  kafka_data: { name: main_kafka_data }
  namenode_data: { name: main_namenode_data }
  datanode_data: { name: main_datanode_data }
  spark_logs: { name: main_spark_logs }
  spark_work: { name: main_spark_work }
  airflow_logs: { name: main_airflow_logs }
  airflow_plugins: { name: main_airflow_plugins }

services:

  # ============================================================
  # 1. PostgreSQL 15-alpine
  # ============================================================
  postgresql:
    image: postgres:15-alpine
    container_name: postgres-main
    hostname: postgresql
    env_file: [ .env ]
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      TZ: ${TZ}
    ports:
      - "${EXTERNAL_POSTGRES_PORT}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 1G }
        reservations: { memory: 512M }
    healthcheck:
      # ✅ FIX ①: $${} → ${} — env_file 변수는 컨테이너 내부에서 $VAR 으로 참조
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ============================================================
  # 1-1. DB 초기화 (최초 1회)
  # ============================================================
  init-db:
    image: postgres:15-alpine
    container_name: init-db-main
    depends_on:
      postgresql: { condition: service_healthy }
    env_file: [ .env ]
    environment:
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_DB: ${POSTGRES_DB}
      AIRFLOW_DB: ${AIRFLOW_DB}
    volumes:
      - ./scripts/init-db.sh:/init-db.sh:ro
    command: [ "sh", "/init-db.sh" ]
    networks: [ main-network ]
    restart: "no"

  # ============================================================
  # 2. MongoDB 7.0
  # ============================================================
  mongodb:
    image: mongo:7.0
    container_name: mongo-main
    hostname: mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGODB_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGODB_PASSWORD}
      TZ: ${TZ}
    ports:
      - "${EXTERNAL_MONGODB_PORT}:27017"
    volumes:
      - mongo_data:/data/db
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 1G }
        reservations: { memory: 512M }
    healthcheck:
      test: [ "CMD-SHELL", "mongosh --eval \"db.adminCommand('ping')\" --quiet" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 15s

  # ============================================================
  # 3. Redis 7-alpine
  # ============================================================
  redis:
    image: redis:7-alpine
    container_name: redis-main
    hostname: redis
    environment:
      TZ: ${TZ}
    command: redis-server --requirepass ${REDIS_PASSWORD}
    ports:
      - "${EXTERNAL_REDIS_PORT}:6379"
    volumes:
      - redis_data:/data
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 512M }
        reservations: { memory: 256M }
    healthcheck:
      # ✅ FIX ①: $${REDIS_PASSWORD} → ${REDIS_PASSWORD}
      test: [ "CMD-SHELL", "redis-cli -a ${REDIS_PASSWORD} ping | grep PONG" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s

  # ============================================================
  # 4. Elasticsearch 8.11.0
  # ============================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch-main
    hostname: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - TZ=${TZ}
    ports:
      - "${EXTERNAL_ELASTICSEARCH_PORT}:9200"
      - "9300:9300"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 2G }
        reservations: { memory: 1G }
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:9200/_cluster/health | grep -qE '\"status\":\"(green|yellow)\"'" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ============================================================
  # 5. Zookeeper (Confluent 7.5.0)
  # ============================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper-main
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
      TZ: ${TZ}
    ports:
      - "${ZOOKEEPER_PORT}:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 512M }
        reservations: { memory: 256M }

  # ============================================================
  # 6. Kafka (Confluent 7.5.0)
  #    내/외부 리스너 분리
  #    · INTERNAL://kafka:9092  — 컨테이너 간 통신
  #    · EXTERNAL://localhost:${KAFKA_PORT}  — 호스트 직접 접근
  # ============================================================
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-main
    hostname: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:${KAFKA_PORT}
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      TZ: ${TZ}
    ports:
      - "${KAFKA_PORT}:9093"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 1G }
        reservations: { memory: 512M }
    healthcheck:
      test: [ "CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 > /dev/null 2>&1" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ============================================================
  # 7. Hadoop NameNode — apache/hadoop:3.3.6
  #
  # ✅ FIX ④: conf 파일 마운트 제거 → command에서 직접 XML 생성
  #    이유: conf 파일이 없으면 Docker가 빈 디렉토리로 마운트해
  #          /opt/hadoop/etc/hadoop/core-site.xml 이 디렉토리가 돼버림
  # ============================================================
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode-main
    hostname: namenode
    user: root
    environment:
      - TZ=${TZ}
    ports:
      - "${HADOOP_NAMENODE_WEBUI_PORT}:9870"
      - "${HDFS_NAMENODE_PORT}:9000"
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 1G }
    command: >
      bash -c "
        mkdir -p /opt/hadoop/etc/hadoop

        echo '<?xml version=\"1.0\"?><configuration><property><name>fs.defaultFS</name><value>hdfs://namenode:9000</value></property><property><name>hadoop.http.staticuser.user</name><value>root</value></property></configuration>' > /opt/hadoop/etc/hadoop/core-site.xml
        
        echo '<?xml version=\"1.0\"?><configuration><property><name>dfs.namenode.name.dir</name><value>/hadoop/dfs/name</value></property><property><name>dfs.replication</name><value>1</value></property><property><name>dfs.permissions.enabled</name><value>false</value></property><property><name>dfs.webhdfs.enabled</name><value>true</value></property><property><name>dfs.namenode.datanode.registration.ip-hostname-check</name><value>false</value></property></configuration>' > /opt/hadoop/etc/hadoop/hdfs-site.xml

        hdfs namenode -format -nonInteractive -force 2>/dev/null || true
        hdfs namenode
      "
    healthcheck:
      # curl 대신 bash 내장 기능으로 9870 포트 확인
      test: [ "CMD-SHELL", "bash -c '</dev/tcp/localhost/9870'" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ============================================================
  # 8. Hadoop DataNode — apache/hadoop:3.3.6
  # ============================================================
  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode-main
    hostname: datanode
    user: root
    depends_on:
      namenode: { condition: service_healthy }
    environment:
      - TZ=${TZ}
    ports:
      - "${HADOOP_DATANODE_WEBUI_PORT}:9864"
    volumes:
      - datanode_data:/hadoop/dfs/data
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 1G }
    command: >
      bash -c "
        mkdir -p /opt/hadoop/etc/hadoop
        echo '<?xml version=\"1.0\"?><configuration><property><name>fs.defaultFS</name><value>hdfs://namenode:9000</value></property><property><name>hadoop.http.staticuser.user</name><value>root</value></property></configuration>' > /opt/hadoop/etc/hadoop/core-site.xml       
        echo '<?xml version=\"1.0\"?><configuration><property><name>dfs.datanode.data.dir</name><value>/hadoop/dfs/data</value></property><property><name>dfs.replication</name><value>1</value></property><property><name>dfs.permissions.enabled</name><value>false</value></property></configuration>' > /opt/hadoop/etc/hadoop/hdfs-site.xml
        hdfs datanode
      "
    healthcheck:
      # 9864 포트 확인
      test: [ "CMD-SHELL", "bash -c '</dev/tcp/localhost/9864'" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ============================================================
  # 9. Spark Master — apache/spark:3.5.3
  #
  # ✅ FIX ③⑤: SPARK_MODE 제거 + spark-defaults.conf 마운트 제거
  #    apache/spark 공식 이미지는 SPARK_MODE 환경변수 미지원 (Bitnami 전용)
  #    spark.hadoop.fs.defaultFS 는 environment로 직접 전달
  # ============================================================
  spark-master:
    image: apache/spark:3.5.3
    container_name: spark-master-main
    hostname: spark-master
    depends_on:
      namenode: { condition: service_healthy }
    environment:
      - SPARK_NO_DAEMONIZE=true
      - TZ=${TZ}
    ports:
      - "${SPARK_MASTER_WEBUI_PORT}:8080"
      - "${SPARK_MASTER_PORT}:7077"
      - "${SPARK_REST_PORT}:6066"
    volumes:
      - ./data-pipeline/spark/jobs:/opt/spark/work-dir/jobs
      - spark_logs:/opt/spark/logs
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 1G }
        reservations: { memory: 512M }
    command: >
      bash -c "
        mkdir -p /opt/spark/conf
        
        echo 'spark.master spark://spark-master:7077' > /opt/spark/conf/spark-defaults.conf
        echo 'spark.hadoop.fs.defaultFS hdfs://namenode:9000' >> /opt/spark/conf/spark-defaults.conf
        echo 'spark.eventLog.enabled false' >> /opt/spark/conf/spark-defaults.conf
        echo 'spark.driver.memory 512m' >> /opt/spark/conf/spark-defaults.conf
        echo 'spark.executor.memory 1g' >> /opt/spark/conf/spark-defaults.conf

        /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      "
    healthcheck:
      # 8080 포트 확인
      test: [ "CMD-SHELL", "bash -c '</dev/tcp/localhost/8080'" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 20s

  # ============================================================
  # 10. Spark Worker — apache/spark:3.5.3
  # ============================================================
  spark-worker-1:
    image: apache/spark:3.5.3
    container_name: spark-worker-1-main
    hostname: spark-worker-1
    depends_on:
      spark-master: { condition: service_healthy }
    environment:
      - SPARK_NO_DAEMONIZE=true
      - TZ=${TZ}
    ports:
      - "${SPARK_WORKER1_WEBUI_PORT}:8081"
    volumes:
      - ./data-pipeline/spark/jobs:/opt/spark/work-dir/jobs
      - spark_work:/opt/spark/work
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 2G }
        reservations: { memory: 1G }
    command: >
      bash -c "
        mkdir -p /opt/spark/conf
        
        echo 'spark.master spark://spark-master:7077' > /opt/spark/conf/spark-defaults.conf
        echo 'spark.hadoop.fs.defaultFS hdfs://namenode:9000' >> /opt/spark/conf/spark-defaults.conf

        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker --cores 2 --memory 2g --webui-port 8081 spark://spark-master:7077
      "

  # ============================================================
  # 11. FastAPI
  # ============================================================
  fastapi:
    image: python:3.11-slim
    container_name: fastapi-main
    hostname: fastapi
    depends_on:
      postgresql: { condition: service_healthy }
      mongodb: { condition: service_healthy }
      redis: { condition: service_healthy }
    env_file: [ .env ]
    environment:
      - APP_ENV=${APP_ENV}
      - POSTGRES_HOST=postgresql
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - MONGODB_HOST=mongodb
      - MONGODB_PORT=27017
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - TZ=${TZ}
      - PYTHONUNBUFFERED=1
    working_dir: /app
    ports:
      - "${FASTAPI_PORT}:8900"
    volumes:
      - .:/app
      - /var/run/docker.sock:/var/run/docker.sock # Docker 모니터링용
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 1G }
        reservations: { memory: 512M }
    command: >
      bash -c "pip install --no-cache-dir -r web/backend/requirements.txt &&
               python -m uvicorn web.backend.app.main:app --host 0.0.0.0 --port 8900 --reload"
    healthcheck:
      test: [ "CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8900/health\")'" ]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s

  # ============================================================
  # 12. Airflow Webserver — apache/airflow:2.10.4-python3.11
  # ============================================================
  airflow-webserver:
    image: apache/airflow:2.10.4-python3.11
    container_name: airflow-webserver-main
    hostname: airflow-webserver
    user: "50000:0"
    depends_on:
      init-db: { condition: service_completed_successfully }
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/${AIRFLOW_DB}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_WEBSERVER_SECRET_KEY}
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__CONN__SPARK_DEFAULT=spark://spark-master:7077
      - TZ=${TZ}
    ports:
      - "${AIRFLOW_WEBSERVER_PORT}:8080"
    volumes:
      - ./data-pipeline/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/airflow/plugins:/opt/airflow/plugins
      - airflow_logs:/opt/airflow/logs
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 2G }
        reservations: { memory: 1G }
    command: >
      bash -c "
        echo '패키지 설치 중...' &&
        pip install --no-cache-dir apache-airflow-providers-apache-spark==4.10.0 apache-airflow-providers-postgres==5.12.0 psycopg2-binary pyspark==3.5.3 &&
        echo 'DB 마이그레이션 중...' &&
        airflow db migrate &&
        echo 'Admin 계정 생성 중...' &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password '${AIRFLOW_ADMIN_PASSWORD}' || true &&
        echo '웹서버 시작!' &&
        airflow webserver
      "
    healthcheck:
      test: [ "CMD-SHELL", "python -c 'import urllib.request; resp = urllib.request.urlopen(\"http://localhost:8080/health\"); exit(0 if b\"healthy\" in resp.read() else 1)'" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  # ============================================================
  # 13. Airflow Scheduler — apache/airflow:2.10.4-python3.11
  # ============================================================
  airflow-scheduler:
    image: apache/airflow:2.10.4-python3.11
    container_name: airflow-scheduler-main
    hostname: airflow-scheduler
    user: "50000:0"
    depends_on:
      airflow-webserver: { condition: service_healthy }
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/${AIRFLOW_DB}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CONN__SPARK_DEFAULT=spark://spark-master:7077
      - TZ=${TZ}
    volumes:
      - ./data-pipeline/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/airflow/plugins:/opt/airflow/plugins
      - airflow_logs:/opt/airflow/logs
    networks: [ main-network ]
    restart: unless-stopped
    deploy:
      resources:
        limits: { memory: 2G }
        reservations: { memory: 1G }
    command: >
      bash -c "
        pip install --no-cache-dir
          apache-airflow-providers-apache-spark==4.10.0
          apache-airflow-providers-postgres==5.12.0
          psycopg2-binary
          pyspark==3.5.3 &&
        airflow scheduler
      "
    healthcheck:
      test: [ "CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$$HOSTNAME\"" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
  # ============================================================
  # 14. Airflow Triggerer — apache/airflow:2.10.4-python3.11
  # ============================================================
  # airflow-triggerer:
  #   image: apache/airflow:2.10.4-python3.11
  #   container_name: airflow-triggerer-main
  #   hostname: airflow-triggerer
  #   user: "50000:0"
  #   depends_on:
  #     airflow-webserver: { condition: service_healthy }
  #   environment:
  #     - AIRFLOW_HOME=/opt/airflow
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/${AIRFLOW_DB}
  #     - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
  #     - TZ=${TZ}
  #   volumes:
  #     - ./data-pipeline/airflow/dags:/opt/airflow/dags
  #     - airflow_logs:/opt/airflow/logs
  #   networks: [main-network]
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:       { memory: 512M }
  #       reservations: { memory: 256M }
  #   command: >
  #     bash -c "
  #       pip install --no-cache-dir
  #         apache-airflow-providers-apache-spark==4.10.0
  #         psycopg2-binary &&
  #       airflow triggerer
  #     "
  #   # healthcheck:
  #   #   test: ["CMD-SHELL", "airflow jobs check --job-type TriggererJob --hostname \"$$HOSTNAME\" --limit 1"]
  #   #   interval: 30s
  #   #   timeout: 10s
  #   #   retries: 5
  #   #   start_period: 180s

  # ================================================================
  #  .env 필수 추가 항목 (start_all.sh가 자동 생성)
  # ================================================================
  #  AIRFLOW_FERNET_KEY=<자동생성>
  #  AIRFLOW_WEBSERVER_SECRET_KEY=<자동생성>
  #  AIRFLOW_ADMIN_PASSWORD=${ADMIN_PASSWORD}   ← .env의 ADMIN_PASSWORD에서 자동 설정
  #  AIRFLOW_DB=airflowdb          ← 기존 .env에 이미 존재

  # ================================================================
  #  서비스 기동 순서 (의존성 체인)
  # ================================================================
  #
  #  postgresql ──→ init-db ──→ airflow-webserver ──→ airflow-scheduler
  #                                               └──→ airflow-triggerer
  #  postgresql ──→ fastapi
  #  mongodb    ──→ fastapi
  #  redis      ──→ fastapi
  #  zookeeper  ──→ kafka
  #  namenode   ──→ datanode
  #  namenode   ──→ spark-master ──→ spark-worker-1
